```ad-def
title: Agenti risolutori "classici"
Gli agenti risolutori di problemi "classici" assumono:
- Ambienti completamente osservabili e deterministici
- Si trovano nelle condizione di produrre offline una sequenza di azioni che può essere eseguito senza imprevisti per raggiungere l'obiettivo.
```

La ricerca sistematica (o euristica) nello spazio di stati è troppo costosa $\implies$ metodi di ricerca locale.

## Ricerca locale
```ad-warning
title: Assunzioni per la ricerca locale
Fin'ora abbiamo visto algoritmi che esplorano gli spazi di ricerca alla ricerca di un goal e restituiscono un cammino soluzione.
Tuttavia a volte lo stato goal è la soluzione del problema.
```

Gli algoritmi di ricerca locale sono adatti per problemi in cui:
- La sequenza di azioni non è importante, è importante unicamente lo stato goal.
- Tutti gli elementi della soluzione sono nello stato ma alcuni vincoli sono violati. Ci interessa ottenere la soluzione, non il path.

```ad-def
title: Algoritmi di ricerca locale
- Non sono sistematici
- Tengono traccia solo del nodo corrente e si spostano su nodi adiacenti (al contrario di un algoritmo globale).
- Non tengono traccia dei cammini (non servono in uscita).
	1. Sono efficienti in termini di occupazione di memoria
	2. Possono trovare soluzioni ragionevoli anche in spazi molto grandi e infiniti (e.g spazi continui).
- Utili per risolvere problemi di ottimizzazione:
	- Stato migliore secondo una funzione obiettivo $f$.
	- Stato di costo minore (ma non il path, cerchiamo lo stato di costo minore, non il percorso di costo minore).

e.g minimizzare il numero di regine sotto attacco.

```

### Panorama dello spazio degli stati
Uno stato ha una posizione sulla superficie e un'altezza che corrisponde al valore della funzione di valutazione (funzione obiettivo).

![[panorama_sstati.png]]

$$f =\text{euristica di costo della funzione obiettivo}$$ Ci spostiamo di stato in stato e ogni volta viene rivalutata la funzione obiettivo. Un algoritmo provoca movimento sulla superficie.
- Vogliamo trovare l'avvallamento più basso (e.g minimizzare il costo) o il picco più alto (e.g massimo di un obiettivo).

### Ricerca in salita (Hill climbing, steepest-ascent/descent)
L'algoritmo di ricerca hill-climbing (versione steepest-ascent) non è altro che un ciclo che si muove continuamente in direzione di un valore crescente, cioè in salita. 
- Termina quando raggiunge un "picco" in cui nessun vicino ha un valore superiore. 
- L'algoritmo non mantiene un albero di ricerca, quindi la struttura dei dati per il nodo corrente deve registrare solo lo stato e il valore della funzione obiettivo. 
- L'Hill climbing non guarda oltre i vicini immediati dello stato corrente. È come cercare di trovare la cima dell'Everest in una fitta nebbia mentre si soffre di amnesia.

Abbiamo diverse implementazioni di questo algoritmo di ricerca che si differenziano per il modo in cui viene scelto un nodo tra quelli generati (in modo tale da migliorare la valutazione dello stato attuale):
- Seleziono il migliore $\implies$ Hill climbing a salita ripida.
- Seleziono uno a caso (tra quelli che salgono) $\implies$ Hill climbing stocastico.
- Seleziono il primo generato $\implies$ Hill climbing con prima scelta.

Se non ci sono stati successori migliori, allora l'algoritmo termina con fallimento.

```ad-note
title: Greediness
L'Hill Climbing è talvolta chiamato ricerca locale greedy, in quanto afferra uno stato di buon vicinato senza pensare a dove andare dopo. L'Hill Climbing fa spesso rapidi progressi verso una soluzione, perché di solito è abbastanza facile migliorare uno stato negativo. 
```

```ad-warning
title: Quando si interrompe l'hill climbing?
- **Massimi locali**. Un massimo locale è un picco che è più alto di ciascuno dei suoi stati vicini ma più basso del massimo globale. Gli algoritmi di Hill-climbing che raggiungono l'intorno di un massimo locale saranno attratti verso l'alto in direzione del picco, ma saranno poi bloccati senza poter andare da nessuna parte.
- **Creste**. Le creste danno luogo a una sequenza di massimi locali che è molto difficle da percorrere per gli algoritmi greedy.
- **Plateaux**. Un plateau è un'area piatta dello spazio degli stati. Può essere un massimo locale piatto, da cui non esiste un'uscita in salita da cui è possibile progredire. Una ricerca di tipo hill-climbing potrebbe perdersi su un plateau.
```
#### Codice
```python
def hill_climbing(problem):
	current = Node(problem.initial_state)
	while True:
		vicini = [current.child_node(problem, action) 
			for action in problem.actions(current.state)]
		if not vicini:
			# current non ha successori
			# esci e ritorna current
			break
		# scegli il vicino con valore più alto (sulla funzione problem.value)
		vicino = (sorted(vicini, key = lambda x: problem.value(x), reverse = True))[0]
		if problem.value(vicino) <= problem.value(current):
			# non miglioro lo stato
			break
		# il vicino è migliore, aggiorno current
		current = vicino
	return current
	
```

#### Problema delle 8 regine (formulazione a stato completo)
Gli algoritmi di ricerca locale utilizzano in genere una formulazione a stati completi, in cui ogni stato ha 8 regine sulla scacchiera, una per colonna. I successori di uno stato sono tutti i possibili stati generati dallo spostamento di una singola regina in un'altra casella della stessa colonna (quindi ogni stato ha $8 \cdot 7 = 56 successori$). La funzione di costo euristico $h$ (stima euristica del costo $f$) è il numero di coppie di regine che si attaccano a vicenda, direttamente o indirettamente. 

![[8regine_hillclimbing_ex1.png]]
- Il minimo globale di questa funzione è zero, che si verifica solo in presenza di soluzioni perfette. La Figura mostra uno stato con $h = 17$, all'interno delle caselle mostra i valori di tutte le coppie di regine che si attaccano reciprocamente. La figura mostra anche i valori di tutti i suoi successori, con i migliori successori che hanno h=12. Gli algoritmi di Hill-climbing scelgono tipicamente in modo casuale tra l'insieme dei migliori successori, se ce n'è più di uno.

![[8regine_hillclimbing_ex2.png]]
- In questo caso abbiamo $h = 1$, tutti gli stati successori non migliorano la soluzione.
- Per le 8 regine l'algoritmo si blocca l'$86\%$ delle volte.
- Però funziona rapidamente, impiegando in media solo 4 passi quando riesce e 3 quando si blocca. Non male per uno spazio di stati con $8^8 \sim 17$ milioni di stati.

![[8regine_hillclimbing_ex3.png]]
- In questo caso in sole 3 mosse siamo riusciti a minimizzare la funzione obiettivo, passando da $h = 5$ (perché si stanno attaccando 5 coppie) fino a 0 (minimo assoluto).

#### Miglioramenti
1. Consentire, un numero limitato di, mosse laterali. Ossia ci si ferma per minore stretto $<$ nell'algoritmo piuttosto che per $\leq$, quindi continua anche a parità di $h$; questo perché magari dopo un certo plateau si può salire ancora (rilassiamo un vincolo).
2. **Hil-climbing stocastico**. si sceglie a caso tra le mosse in salita (magari tenendo conto della pendenza).
	- Converge più lentamente ma a volte trova soluzioni migliori.
3. **Hill-climbing con prima scelta**. può generare le mosse a caso, uno alla volta, fino a trovarne una migliore dello stato corrente (si prende solo il primo che migliora).
	- Come la stocastica ma utile quando i successori sono molti (nell'ordine delle migliaia o più) evitando una scelta tra tutti.
4. **Hill-climbing con riavvio casuale** (random restart): *"Se all'inizio non ci riuscite, provate e riprovate"*. Si tratta di una serie di ricerche di tipo hill-climbing a partire da stati iniziali generati in modo casuale, fino a trovare un goal.
	- Se la probabilità di successo è $p\implies$ saranno necessarie in media $$\dfrac{1}{p}\text{ ripartenze}$$ per trovare la soluzione.
	- Questa variante è nota per essere tendenzialmente completa (e.g insistendo si generano tutte).
	- Il suo successo dipende molto dalla forma del panorama degli stati:
		- Panorama frastagliato $\implies$ molti massimi locali $\implies$ si inceppa facilmente.

```ad-def
title: Tempra simulata
L'algoritmo di tempra simulata (Simulated annealing) combina l'hill-climbing con una scelta stocastica (non del tutto casuale per via dell'efficienza). In soldoni è una sofisticazione dell'approccio stocastico.
- Il nome è analogo al processo di tempra dei metalli in metallurgia:
> Se c’è un difetto reticolare (stato che non ci piace) riscaldano il metallo (molto caos) per poi raffreddarlo pian piano per raggiungere la configurazione desiderata.

```

```ad-def 
title: Tempra simulata (per ascesa)
Ad ogni passo si sceglie un successore $u'$ casuale:
- Se migliora lo stato corrente $\implies u'$ viene espanso
- Altrimenti, caso in cui $$\Delta E = f(u') - f(u) \leq 0$$ quel nodo viene scelto con probabilità $$p = \exp{(\Delta E / T)}, \;\; 0\leq p \leq 1$$ Cioè, se siamo nel caso peggiorativo o uguale possiamo proseguire con un successore con una certa probabilità, altrimenti no.

 Come possiamo notare:
 - $p$ è inversamente proporzionale al peggioramento, infatti se la mossa peggiora molto (cioè $\Delta E$ diventa molto negativo) allora la $p$ si abbassa (probabilità minore).
 - La temperatura $T$ decresce col progradire dell'algoritmo (quindi anche $p$) secondo una strategia definita:
> Col progredire rende improbabili le mosse peggiorative.
> - Se $T$ alta (situazione iniziale) abbiamo $e^x$ con $x$ molto vicino a 0, quindi la probabilità è molto vicina a 1. Accetto configurazioni molto più facilmente.
> - Se $T$ bassa $e^x$ tende a infinito quindi diventa molto meno probabile.
```

```ad-note
title: Analisi Tempra Simulata
La probabilità $p$ di una mossa in discesa diminuisce col tempo e l'algoritmo si comporta sempre di più come Hill Climbing (con temperature basse, vicino a zero, si comporta come Hill Climbing).
- Se $T$ viene decrementato abbastanza lentamente con $p\to1$ si raggiunge la soluzione ottimale.
```

```ad-note
title: Parametri tempra simulata
Valore iniziale e decremento di $T$ sono parametri.
- I valori per $T$ vengono determinati sperimentalmente, inizialmente $$T \text{ tale che per valori medi di }\Delta E \text{ vale che }\exp{(\Delta E / T)} \sim 0.5$$
```
### Ricerca local beam
È la versione locale della beam search: si tengono in memoria $k$ stati, anziché uno solo.
- Ad ogni passo si generano i successori di tutti i $k$ stati.
	- Se si trova un goal ci si ferma.
	- Altrimenti si prosegue con i $k$ migliori tra questi successori.

### Beam search stocastica
Si introduce un elemento di casualità, come in un processo di selezione naturale (in modo tale da diversificare le future generazioni).
- Nella variante stocastica della local beam, si scelgono $k$ successori ma con probabilità maggiore per i migliori (non necessariamente si scelgono i migliori).

$$organismo \equiv stato$$
$$\textit{popolazione di individui} \equiv stati$$
$$progenie \equiv successori$$
$$fitness \text{(il valore della f)} \equiv idoneità$$

### Algoritmi genetici/evolutivi
```ad-def
title: Idea
Sono varianti della beam search stocastica in cui gli stati successori sono ottenuti combinando due stati genitore (anziché per evoluzione).
> Non si evolve una generazione dietro l’altra, due genitori procreano nuovi successori accoppiandosi.
```

```ad-def
title: Funzionamento
Abbiamo una popolazione iniziale:
- $k$ stati/individui vengono generati casualmente.
- Ogni individuo è rappresentato come una stringa. e.g `"24748552"` posizione nelle colonne nel problema delle 8 regine.
- Gli individui sono valutati da una funzione di fitness. e.g $\#\text{coppie di regine che non si attaccano}$.
- Si selezionano gli individui per gli accoppiamenti con una probabilità proporzionale alla fitness.
- Le coppie danno vita alla generazione successiva.
	- Combinando materiale genetico (**crossover**)
	- Con un meccanismo aggiuntivo di mutazione genetica casuale.
- La popolazione ottenuta dovrebbe essere migliore.
- La cosa si ripete fino a ottenere stati abbastanza buoni (stati obiettivo) o finché si raggiunge un "plateau" a livello di miglioramenti.
```

e.g:

![[8regine_algGenetico_ex.png]]

> Per ogni coppia (scelta con probabilità proporzionale alla fitness) viene: 
> - scelto un punto di crossing over e vengono generati due figli scambiandosi pezzi (del DNA).
> - in seguito viene effettuata una mutazione casuale che dà luogo alla prossima generazione.
> 
> La fitness progressivamente tenderà a favorire generazioni migliori.

Quindi:

- Tendenzialmente prendiamo le configurazioni con miglior fitness. 
- La zona di crossing over viene utilizzata per discriminare quali “geni” prendere da un genitore e quali dall’altro, così da poter sviluppare un figlio che ha le migliori caratteristiche di entrambi. 
- La mutazione avviene casualmente.

#### Nascita di un figlio

![[nascita_figlio_scacchi.png]]

- Le parti chiare sono passate al figlio.
- Le parti grigie si perdono.
- Se i genitori sono molto diversi anche i nuovi stati sono diversi.
- All'inizio vi sono spostamenti maggiori che poi si raffinano.
	- Maggiori perché c’è molta eterogeneità, man mano che si va’ avanti con il fitness ci saranno meno spostamenti.

### Algoritmi genetici
