```ad-def
title: Support Vector Machine (SVM)
È un classificatore derivato dalla [[5. Validation and Theoretical issues#Statistical Learning Theory (SLT)|Statistical Learning Theory]] di Vapnik. Dopo anni di sviluppo teorico, la SVM acquisì fama per l'uso, singolare, di immagini come input che la rendeva accurata quasi quanto una rete neurale del tempo (1990) con task di riconoscimento calligrafico.

- Oggi viene usata molto nell'ambito dell'apprendimento supervisionato (e la regressione).
```

## Maximum margin classifier
### Hard margin SVM
In questo primo caso consideriamo un problema di classificazione binaria, linearmente separabile con dati di training non rumorosi. 

> Queste ipotesi sono forti, per questo motivo di **hard margin**.

Non tutti gli iperpiani sono uguali, cambiando l'iperpiano cambia anche il margine.

```ad-def
title: Margine
Il margine è il doppio della distanza tra l'iperpiano separatore e il punto più vicino tra gli esempi di input.

![[margine.png]]

- Si preferisce l'iperpiano che **massimizza il margine**.
- Più i punti sono **lontani** dall'iperpiano più il rischio di mal classificare un input è **basso**, in quanto i punti sono ben distinti.
```

![[margine_safezone.png]]

Consideriamo il margine come una safe zone, tenendo ben margine rischio meno di sbagliare la classificazione di un dato.

```ad-def
title: Iperpiano
È il luogo dei punti per cui $$w^{T} x + b = 0$$
```

```ad-def
title: Margine superiore
È il luogo dei punti per cui $$w^{T} x + b = -1$$
```

```ad-def
title: Vettori di supporto
Si dicono *vettori di supporto* sono i punti più vicini al *decision boundary*, formalmente sono i punti che si trovano sul marigine inferiore o superiore, cioè i punti $$x_{p}\text{ tale che }\quad \lvert w^{T} x_{p} + b\rvert = 1$$

![[support_vector.png]]

n.b: $b = w_{0}$
```

Consideriamo il problema dell'apprendimento di un modello lineare per classificazione binaria, cioè trovare una funzione $$h:\mathbb R ^ {n} \to \{ -1,+1 \} \mid h(x) = \text{sign}(wx + b)$$  a partire da un training set formato da esempi nella forma $(x_{p},y_{p})$.

Il task consiste nel trovare $(w,b)$  tale che tutti i punti siano correttamente classificati e il margine massimizzato.

Tutti i punti del training set sono correttamente classificati quando nessun punto si trova sul boundary $$\begin{cases}
w^{T}x + b \geq 0\quad &\text{se }y_{p}= 1\\
w^{T}x + b \leq -1\quad &\text{se }y_{p}= -1
\end{cases} \quad\forall\; p \in \{1,\ldots,n\}$$ cioè $(w^{T}x + b)y_{p} \geq 1 \;\forall p\in\{ 1,\ldots,n \}$

Soddisfando tutti i vincoli sui punti di training, essi sono correttamente classificati e l'errore empirico è 0.

```ad-note
title: Nozioni utili
- La dimensione del margine è proporzionale a $$\frac{2}{\lvert\lvert w \rvert\rvert}$$ cioè **inversamente proporzionale** alla norma di $w$, quindi massimizzare il margine significa minimizzare la norma $\lvert \lvert w \rvert \rvert$, cioè minimizzare $\dfrac{\lvert\lvert w \rvert\rvert^2}{2}$.
- La [[5. Validation and Theoretical issues#Vapnik-Chervonenkis dim|VC-dimension]] dell'SVM è inversamente proporzionale al margine.
	- La VC-dim decresce **al crescere** del margine.
	- Massimizzando il margine, minimizziamo la VC-dim $\implies$ ottenendo una giusta classificazione.
```

In soldoni: 
> Se tendiamo a massimizzare il margine, tendiamo a mantenere bassa la complessità del modello e anche a mantenere bassi i valori di $w$. Quindi non si va in overfitting.


Nella formulazione del problema in **forma primale**, la funzione obiettivo è $$\min\; \dfrac{\lvert\lvert w\rvert\rvert^2}{2}$$ e i vincoli sono $$\left( w^{T}x + b \right)y_{p} \geq 1\quad\forall p=1\ldots l$$ 
- Il modello è costruito direttamente tenendo conto della complessità, con funzione obiettivo ed errore su training set nullo.
- Minimizzando quella funzione obiettivo, come detto poc'anzi, massimizziamo il margine (leggere *nozioni utili*).
	- Con i vincoli ci assicuriamo che tutti i punti del training set siano correttamente classificati.

In realtà noi risolveremo il problema in forma duale: $$\begin{align*}
\max\;\sum\limits_{i} \alpha_{i}- \sum\limits_{i,j} \alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{t}/2& \quad\forall\;i,j=1\ldots l\\
\alpha_{i}\geq&0\\
\sum\limits_{i}\alpha_{i}y_{i}=&0
\end{align*}$$ vogliamo trovare le soluzioni ottime $\alpha_{p}$ per $p\in \{1,\ldots,n\}$, cioè i moltiplicatori di Lagrange, una volta trovati gli $\alpha$ possiamo calcolare $(w,b)$
$$\begin{align*}
w &= \sum\limits_{p} \alpha_{p}y_{p}x_{p}\\
b &= y_{k}-w^{T}x_{k}\quad\forall\;\alpha_{k}>0 \\
h(x) &= \text{sign}(w^{T}x + b) = \text{sign}\left( \sum\limits_{p=1}^{l}\alpha_{p}y_{p}x_{p}^{T}x + b \right)
\end{align*}$$ 
sapendo che $\alpha_{p}\neq0\implies x_{p}\text{ vettore supporto}$, la soluzione è 
1. spesso sparsa
2. formulata solo in termini dei vettori di supporto

quindi l'iperpiano dipende soltanto dai vettori di supporto (anche se per trovare i valori di $\alpha$ servoro comunque tutti i dati del training set).

Per queto motivo possiamo scrivere la forma $h(x)$ in una forma diversa, in cui la sommatoria è solo sui vettori di supporto (il cui insieme chiameremo $SV$). $$h(x) = \text{sign}\left( \sum\limits_{p\in SV}\alpha_{p}y_{p}x_{p}^{T}x + b \right)$$ 
- Sono i vettori di supporto a determinare la soluzione.

![[iperpiano_supportVector.png]] 

In figura notiamo come l'eliminazione di due punti verdi casuali, che non siano *support vector*, non cambia la retta. Risultato analogo se vengono presi in considerazione due punti bianchi (l'importante è che non siano support vector).

```ad-theo
title: Regola del prodotto scalare 
Il prodotto scalare mostra quanto l'input e il $p$-esimo pattern si somiglino, tale valore è pesato con $\alpha_{p}$ e $y_{p}$. $$h(x) = \text{sign}\left( \sum\limits_{p\in SV}\alpha_{p}y_{p}\quad \mathbf{x_{p}^{T}x} \quad + b \right)$$
```

### Soft Margin SVM
Richiedere che tutti i punti ricadano all'esterno del margine può essere **troppo restrittivo**, potremmo pensare di ammettere alcuni errori sia per tollerare la presenza di dati rumorosi che per aumentare la dimensione del margine.
Per fare ciò introduciamo delle **slack variables**.

![[slack_var1.png]]

In una situazione del genere, per separare le *"x"* e le *"o"* dovrei avere un margine piccolissimo (quello arancione), mentre i due punti più restrittivi (quindi ammettendo errori) favorirei un margine molto più ampio (cioè quello blu).

```ad-def
title: Slack variable
Le slack variables non sono altro che variabili introdotte all'interno della SVM per permette un certo grado di errore di classificazione dei dati.

- Una slack variable $\xi_{i}$ per un certo punto di training è la distanza tra il punto e il margine corrispondente.
	- Se il punto è classificato come positivo, la variabile è uguale alla distanza tra il punto e $w^{T}x + b = 1$.
	- Se il punto è classificato come negativo, la variabile è uguale alla distanza tra il punto e $w^{T}x + b = -1$.
```

Se una slack variable $\xi_{i}$ ha valore minore di 1 allora c'è errore **ma** il punto si trova comunque dalla parte giusta dell'iperpiano.

![[slack_var2.png]]

Nell'esempio $\xi_{2}$  il punto è classificato bene (sta nel lato giusto dell'iperpiano) ma si trova all'interno del margine. 
La situazione cambia per $\xi_{1}$, il punto è mal classificato. Formalmente $$\begin{align*}
\xi_{1}:&\quad w^{T}x+b=1\\
\xi_{2}:&\quad w^{T}x+b=-1
\end{align*}$$


