Table of contents

1. [[#Maximum margin classifier|Maximum margin classifier]]
	1. [[#Maximum margin classifier#Hard margin SVM|Hard margin SVM]]
	1. [[#Maximum margin classifier#Soft Margin SVM|Soft Margin SVM]]
	1. [[#Maximum margin classifier#Parliamo del kernel|Parliamo del kernel]]
1. [[#Pratica|Pratica]]
	1. [[#Pratica#Model selection|Model selection]]
	1. [[#Pratica#Conclusioni|Conclusioni]]


```ad-def
title: Support Vector Machine (SVM)
È un classificatore derivato dalla [[5. Validation and Theoretical issues#Statistical Learning Theory (SLT)|Statistical Learning Theory]] di Vapnik. Dopo anni di sviluppo teorico, la SVM acquisì fama per l'uso, singolare, di immagini come input che la rendeva accurata quasi quanto una rete neurale del tempo (1990) con task di riconoscimento calligrafico.

- Oggi viene usata molto nell'ambito dell'apprendimento supervisionato (e la regressione).
```

## Maximum margin classifier
### Hard margin SVM
In questo primo caso consideriamo un problema di classificazione binaria, nell'ipotesi di
1. dati di training non rumorosi. 
2. insiemi linearmente separabili

> Queste ipotesi sono forti, per questo motivo si chiama **hard margin**.

Non tutti gli iperpiani sono uguali, cambiando l'iperpiano cambia anche il margine.

```ad-def
title: Margine
Il margine è il doppio della distanza tra l'iperpiano separatore e il punto più vicino tra gli esempi di input.

![[margine.png]]

- Si preferisce l'iperpiano che **massimizza il margine**.
- Più i punti sono **lontani** dall'iperpiano più il rischio di mal classificare un input è **basso**, in quanto i punti sono ben distinti.
```

![[margine_safezone.png]]

Consideriamo il margine come una safe zone, tenendo ben margine rischio meno di sbagliare la classificazione di un dato.

```ad-def
title: Iperpiano
È il luogo dei punti per cui $$w^{T} x + b = 0$$
```

```ad-def
title: Margine superiore
È il luogo dei punti per cui $$w^{T} x + b = 1$$
```

```ad-def
title: Margine inferiore
È il luogo dei punti per cui $$w^{T} x + b = -1$$
```

![[support_vector.png]]

```ad-def
title: Vettori di supporto
Si dicono *vettori di supporto* i punti più vicini al *decision boundary*, formalmente sono i punti che si trovano sul margine inferiore o superiore, cioè i punti $$x_{p}\text{ tale che }\quad \lvert w^{T} x_{p} + b\rvert = 1$$

n.b: $b = w_{0}$
```

Consideriamo il problema dell'apprendimento di un modello lineare per classificazione binaria, cioè trovare una funzione $$h:\mathbb R ^ {n} \to \{ -1,+1 \} \mid h(x) = \text{sign}(wx + b)$$  a partire da un training set formato da esempi nella forma $(x_{p},y_{p})$.

Il task consiste nel trovare $(w,b)$  tale che tutti i punti siano correttamente classificati e il margine massimizzato.

Tutti i punti del training set sono correttamente classificati quando nessun punto si trova sul boundary $$\begin{cases}
w^{T}x + b \geq 0\quad &\text{se }y_{p}= 1\\
w^{T}x + b \leq -1\quad &\text{se }y_{p}= -1
\end{cases} \quad\forall\; p \in \{1,\ldots,n\}$$ cioè $(w^{T}x + b)y_{p} \geq 1 \;\forall p\in\{ 1,\ldots,n \}$

Soddisfando tutti i vincoli sui punti di training, essi sono correttamente classificati e l'errore empirico è 0.

```ad-note
title: Nozioni utili
- La dimensione del margine è proporzionale a $$\frac{2}{\lvert\lvert w \rvert\rvert^2}$$ cioè **inversamente proporzionale** alla norma di $w$, quindi massimizzare il margine significa minimizzare la norma $\lvert \lvert w \rvert \rvert^{2}$, cioè minimizzare $\dfrac{\lvert\lvert w \rvert\rvert^2}{2}$.
- La [[5. Validation and Theoretical issues#Vapnik-Chervonenkis dim|VC-dimension]] dell'SVM è inversamente proporzionale al margine.
	- La VC-dim decresce **al crescere** del margine.
	- Massimizzando il margine, minimizziamo la VC-dim $\implies$ ottenendo una giusta classificazione.
```

In soldoni: 
> Se tendiamo a massimizzare il margine, tendiamo a mantenere bassa la complessità del modello e anche a mantenere bassi i valori di $w$. Quindi non si va in overfitting.


Nella formulazione del problema in **forma primale**, la funzione obiettivo è $$\min\; \dfrac{\lvert\lvert w\rvert\rvert^2}{2}$$ e i vincoli sono $$\left( w^{T}x + b \right)y_{p} \geq 1\quad\forall p=1\ldots l$$ 
- Il modello è costruito direttamente tenendo conto della complessità, con funzione obiettivo ed errore su training set nullo.
- Minimizzando quella funzione obiettivo, come detto poc'anzi, massimizziamo il margine (leggere *nozioni utili*).
	- Con i vincoli ci assicuriamo che tutti i punti del training set siano correttamente classificati.

In realtà noi risolveremo il problema in forma duale: $$\begin{align*}
\max\;\sum\limits_{i} \alpha_{i}- \sum\limits_{i,j} \alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{t}/2& \quad\forall\;i,j=1\ldots l\\
\alpha_{i}&\geq0\\
\sum\limits_{i}\alpha_{i}y_{i}&=0
\end{align*}$$ vogliamo trovare le soluzioni ottime $\alpha_{p}$ per $p\in \{1,\ldots,n\}$, cioè i moltiplicatori di Lagrange, una volta trovati gli $\alpha$ possiamo calcolare $(w,b)$
$$\begin{align*}
w &= \sum\limits_{p} \alpha_{p}y_{p}x_{p}\\
b &= y_{k}-w^{T}x_{k}\quad\forall\;\alpha_{k}>0 \\
h(x) &= \text{sign}(w^{T}x + b) = \text{sign}\left( \sum\limits_{p=1}^{l}\alpha_{p}y_{p}x_{p}^{T}x + b \right)
\end{align*}$$ 
sapendo che $\alpha_{p}\neq0\implies x_{p}\text{ vettore supporto}$, la soluzione è 
1. spesso sparsa
2. formulata solo in termini dei vettori di supporto

quindi l'iperpiano dipende soltanto dai vettori di supporto (anche se per trovare i valori di $\alpha$ servono comunque tutti i dati del training set).

Per questo motivo possiamo scrivere $h(x)$ in una forma diversa, in cui la sommatoria è solo sui vettori di supporto (il cui insieme chiameremo $SV$). $$h(x) = \text{sign}\left( \sum\limits_{p\in SV}\alpha_{p}y_{p}x_{p}^{T}x + b \right)$$  ^5ab9d6
- Sono i vettori di supporto a determinare la soluzione.

![[iperpiano_supportVector.png]] 

In figura notiamo come l'eliminazione di due punti verdi casuali, che non siano *support vector* (quelli cerchiati), non cambia la retta. Risultato analogo se vengono presi in considerazione due punti bianchi (l'importante è che non siano support vector).

```ad-theo
title: Regola del prodotto scalare 
Il prodotto scalare mostra quanto l'input e il $p$-esimo pattern si somiglino, tale valore è pesato con $\alpha_{p}$ e $y_{p}$. $$h(x) = \text{sign}\left( \sum\limits_{p\in SV}\alpha_{p}y_{p}\quad \mathbf{x_{p}^{T}x} \quad + b \right)$$
```

### Soft Margin SVM
Richiedere che tutti i punti ricadano all'esterno del margine può essere **troppo restrittivo**, potremmo pensare di ammettere alcuni errori sia per tollerare la presenza di dati rumorosi che per aumentare la dimensione del margine.
Per fare ciò introduciamo delle **slack variables**.

![[slack_var1.png]]

In una situazione del genere, per separare le *"x"* e le *"o"* dovrei avere un margine piccolissimo (quello arancione), mentre eliminando i due punti più restrittivi (quindi ammettendo errori) favorirei un margine molto più ampio (cioè quello blu).

```ad-def
title: Slack variable
Le slack variables non sono altro che variabili introdotte all'interno della SVM per permette un certo grado di errore di classificazione dei dati.

- Una slack variable $\xi_{i}$ per un certo punto di training è la distanza tra il punto e il margine corrispondente.
	- Se il punto è classificato come positivo, la variabile è uguale alla distanza tra il punto e $w^{T}x + b = 1$.
	- Se il punto è classificato come negativo, la variabile è uguale alla distanza tra il punto e $w^{T}x + b = -1$.
```

![[slack_var2.png]]

Nell'esempio $\xi_{2}$  il punto è classificato bene (sta nel lato giusto dell'iperpiano) ma si trova all'interno del margine. 
La situazione cambia per $\xi_{1}$, il punto è mal classificato. Formalmente $$\begin{align*}
\xi_{1}:&\quad w^{T}x+b>1\\
\xi_{2}:&\quad 0<w^{T}x+b<1
\end{align*}$$

Nella formulazione primale del problema la situazione cambia (con l'introduzione delle slack variables). 
$$\begin{align*}
\min\;\frac{\lvert\lvert w \rvert\rvert^{2}}{2} &+ C\cdot\sum\limits_{p}\xi_{p}\\
(w^Tx_{p}+b)y_{p}&\geq 1-\xi_{p}\\\\

\xi_{p}&\geq0\quad\forall\;p=1\ldots l
\end{align*}$$ 

Con l'introduzione dell'iperparametro $C$, riusciamo a dare un peso all'errore e con ciò a *guidare* il numero di errori ammessi. Spesso viene chiamata variabile di *penalty* (perché permette *misclassification* solo su determinati vincoli).
- Nella funzione obiettivo
	- Il primo termine è quello per la regolarizzazione. $$\frac{\lvert\lvert w \rvert\rvert^{2}}{2}$$
	- Il secondo termine regola gli errori ammessi sul training $$C\cdot\sum\limits_{p}\xi_{p}$$
- Nei vincoli ogni pattern ha un proprio <u>margine d'errore</u> $$(w^Tx_{p}+b)y_{p}\geq \underline{1-\xi_{p}}$$

Abbiamo avuto un trade-off, da un lato abbiamo rilassato le ipotesi permettendo margine d'errore, ma dall'altro ora ci troviamo con un iperparametro da gestire (non abbiamo più sicurezze su underfitting e overfitting).
- Se il valore è troppo basso $\implies$ sono ammessi molti errori sul training set.
	- Troppi errori sul training set, VC-dim bassa (modello molto semplice) quindi **underfitting**.
- Se il valore è troppo alto $\implies$ si tende ad avere zero errori sul training set.
	- Errore empirico nullo, VC-dim molto alta (modello molto complesso) quindi **overfitting**.
	- Abbiamo un margine piccolissimo.
	- e.g: $C\to\infty\implies\text{overfitting}$

### Parliamo del kernel
Rilassando anche la seconda ipotesi e considerando problemi non linearmente separabili.
- Possiamo mappare i punti dell'input space in uno spazio di **dimensioni maggiori** detto **feature space**, in cui i punti risultano linearmente separabili

![[feature_space.png]]

$$\phi: \mathbb R^{2}\to \mathbb R^{3}$$
$$\begin{pmatrix}x_{1}, x_{2}\end{pmatrix}^{T}\mapsto\begin{pmatrix}x_{1}^{2},\sqrt2 x_{1}x_{2},x_{2}^{2}\end{pmatrix}^{T}$$

Abbiamo visto questo approccio già nella [[3. Modelli lineari#Linear Basis expansion|LBE]] in cui invece di avere come input $x$ avevamo $\phi(x)$ per gestire task non lineari **rispetto all'input**.

Come visto in precedenza, la LBE può diventare costosa e portare all'overfitting se la dimensione di $\phi(x)$ cresce. 
Con l'approccio del **kernel** possiamo gestire implicitamente il feature space nel contesto in cui la complessità dei modelli :
- dipende dalla dimensione del margine.
- **NON DALLA DIMENSIONE DELL'INPUT**.

In questo modo anche all'aumentare della dimensione del feature space, la complessità del classificatore può essere tenuta bassa.

Consideriamo la formula di $h(x)$ vista già [[#^5ab9d6|qui]], usando però $\phi(x)$ al posto di $x$ 
$$h(x) =\text{sign}\left(\sum\limits_{p\in SV}\alpha_{p}y_{p}\phi(x_{p}^{T})\phi(x) + b\right)$$ non è necessario calcolare il prodotto scalare $\phi(x_{p}^{T})\phi(x)$, infatti possiamo sostituirlo con una funzione kernel, rendendo la funzione $h(x)$ parametrizzabile $$h(x) =\text{sign}\left(\sum\limits_{p\in SV}\alpha_{p}y_{p}\;\text{K}(x_{p},x) + b\right)$$

```ad-def
title: Kernel
Un kernel $k:\mathbb R^{n}\times\mathbb R^{n}\to \mathbb R$ è una funzione tale per cui esiste uno spazio Hilbertiano $X$ e una funzione $\phi:\mathbb R^{n}\to X$ (con $n$ molto grande), e si ha $$k(x_{i},x_{j}) = \phi(x_{i})^{T}\phi(x_{j})$$
```

Alcuni kernel popolari sono:
- **Lineare** $$k(x_{i},x_{j}) = x_{i}^{T}x_{j}$$
- **Polinomiale** ($\phi(x)$ ha dimensione polinomiale in $k$) $$k(x_i,x_j)=(1+x_{i}^{T}x_{j})^{k}$$
- **Radial Basis Function (RBF)** $$k(x_{i},x_{j})=\exp{\left( -\frac{\lvert\lvert x_{i}-x_{j} \rvert\rvert^2}{2\sigma^2} \right)}$$

L'RBF usa una gaussiana, notiamo che ha un iperparametro $\sigma$:
- Per $\sigma$ piccolo, il kernel ha un picco più alto e stretto intorno a zero.
	- Pattern e input sono simili solo se sono molto vicini tra loro.
	- Il classificatore risponde con la classe assegnata al punto più vicino.
	- Utile per discriminare punti del Training set ma può causare overfitting.

Scegliamo quindi un trade off tra il parametro $C$, la funzione kernel $K$ e i suoi iperparametri. Dopo aver risolto questo problema di ottimizzazione troviamo i valori di $\alpha$ da usare nel modello finale $$h(x) =\text{sign}\left(\sum\limits_{p\in SV}\alpha_{p}y_{p}\;\text{K}(x_{p},x) + b\right)$$ in questo modo il costo computazionale cresce con $l$, cioè con il numero di dati **e non con** $n$ (dimensione del feature space). Inoltre è modulare! Possiamo usare lo stesso classificatore cambiando il kernel.

## Pratica
> Come nella vita, evitiamo di usare qualcosa con aspettative troppo ottimistiche 🫠

Con la SVM non si scappa dall'overfitting, può sempre verificarsi se le componenti del modello
- C
- Kernel
- Parametri del Kernel

non sono scelte correttamente. Inoltre facendo crescere troppo $l$, aggiungendo variabili di input inutili alla classificazione, la complessità del modello cresce.

Infine bisogna usare le tecniche di validazione viste. La scelta delle componenti avviene sul **validation set** e il test anche in questo caso deve essere fatto su un insieme diverso di dati.

![[svm_practice.png]]

### Model selection
Per il model selection, di $C$ e $\gamma$ in RBF che dipende completamente da $\sigma$, prima si considera una ricerca a grana grossa per trovare gli intervalli migliori. Una volta trovati, si ricerca a grana più fine all'interno di essi.

### Conclusioni
SVM è un tool molto utile e buono a livello di performance **MA** 
> Quando hai un martello in mano, tutti i problemi diventano chiodi.

non è sempre la scelta migliore. È comunque valido perché combina LBE con un approccio che cerca di massimizzare il margine, quindi unisce flessibilità a controllo della complessità.