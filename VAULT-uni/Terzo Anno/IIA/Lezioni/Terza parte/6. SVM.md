Table of contents

1. [[#Maximum margin classifier|Maximum margin classifier]]
	1. [[#Maximum margin classifier#Hard margin SVM|Hard margin SVM]]
	1. [[#Maximum margin classifier#Soft Margin SVM|Soft Margin SVM]]
	1. [[#Maximum margin classifier#Parliamo del kernel|Parliamo del kernel]]
1. [[#Pratica|Pratica]]
	1. [[#Pratica#Model selection|Model selection]]
	1. [[#Pratica#Conclusioni|Conclusioni]]


```ad-def
title: Support Vector Machine (SVM)
√à un classificatore derivato dalla [[5. Validation and Theoretical issues#Statistical Learning Theory (SLT)|Statistical Learning Theory]] di Vapnik. Dopo anni di sviluppo teorico, la SVM acquis√¨ fama per l'uso, singolare, di immagini come input che la rendeva accurata quasi quanto una rete neurale del tempo (1990) con task di riconoscimento calligrafico.

- Oggi viene usata molto nell'ambito dell'apprendimento supervisionato (e la regressione).
```

## Maximum margin classifier
### Hard margin SVM
In questo primo caso consideriamo un problema di classificazione binaria, nell'ipotesi di
1. dati di training non rumorosi. 
2. insiemi linearmente separabili

> Queste ipotesi sono forti, per questo motivo si chiama **hard margin**.

Non tutti gli iperpiani sono uguali, cambiando l'iperpiano cambia anche il margine.

```ad-def
title: Margine
Il margine √® il doppio della distanza tra l'iperpiano separatore e il punto pi√π vicino tra gli esempi di input.

![[margine.png]]

- Si preferisce l'iperpiano che **massimizza il margine**.
- Pi√π i punti sono **lontani** dall'iperpiano pi√π il rischio di mal classificare un input √® **basso**, in quanto i punti sono ben distinti.
```

![[margine_safezone.png]]

Consideriamo il margine come una safe zone, tenendo ben margine rischio meno di sbagliare la classificazione di un dato.

```ad-def
title: Iperpiano
√à il luogo dei punti per cui $$w^{T} x + b = 0$$
```

```ad-def
title: Margine superiore
√à il luogo dei punti per cui $$w^{T} x + b = 1$$
```

```ad-def
title: Margine inferiore
√à il luogo dei punti per cui $$w^{T} x + b = -1$$
```

![[support_vector.png]]

```ad-def
title: Vettori di supporto
Si dicono *vettori di supporto* i punti pi√π vicini al *decision boundary*, formalmente sono i punti che si trovano sul margine inferiore o superiore, cio√® i punti $$x_{p}\text{ tale che }\quad \lvert w^{T} x_{p} + b\rvert = 1$$

n.b: $b = w_{0}$
```

Consideriamo il problema dell'apprendimento di un modello lineare per classificazione binaria, cio√® trovare una funzione $$h:\mathbb R ^ {n} \to \{ -1,+1 \} \mid h(x) = \text{sign}(wx + b)$$  a partire da un training set formato da esempi nella forma $(x_{p},y_{p})$.

Il task consiste nel trovare $(w,b)$  tale che tutti i punti siano correttamente classificati e il margine massimizzato.

Tutti i punti del training set sono correttamente classificati quando nessun punto si trova sul boundary $$\begin{cases}
w^{T}x + b \geq 0\quad &\text{se }y_{p}= 1\\
w^{T}x + b \leq -1\quad &\text{se }y_{p}= -1
\end{cases} \quad\forall\; p \in \{1,\ldots,n\}$$ cio√® $(w^{T}x + b)y_{p} \geq 1 \;\forall p\in\{ 1,\ldots,n \}$

Soddisfando tutti i vincoli sui punti di training, essi sono correttamente classificati e l'errore empirico √® 0.

```ad-note
title: Nozioni utili
- La dimensione del margine √® proporzionale a $$\frac{2}{\lvert\lvert w \rvert\rvert^2}$$ cio√® **inversamente proporzionale** alla norma di $w$, quindi massimizzare il margine significa minimizzare la norma $\lvert \lvert w \rvert \rvert^{2}$, cio√® minimizzare $\dfrac{\lvert\lvert w \rvert\rvert^2}{2}$.
- La [[5. Validation and Theoretical issues#Vapnik-Chervonenkis dim|VC-dimension]] dell'SVM √® inversamente proporzionale al margine.
	- La VC-dim decresce **al crescere** del margine.
	- Massimizzando il margine, minimizziamo la VC-dim $\implies$ ottenendo una giusta classificazione.
```

In soldoni: 
> Se tendiamo a massimizzare il margine, tendiamo a mantenere bassa la complessit√† del modello e anche a mantenere bassi i valori di $w$. Quindi non si va in overfitting.


Nella formulazione del problema in **forma primale**, la funzione obiettivo √® $$\min\; \dfrac{\lvert\lvert w\rvert\rvert^2}{2}$$ e i vincoli sono $$\left( w^{T}x + b \right)y_{p} \geq 1\quad\forall p=1\ldots l$$ 
- Il modello √® costruito direttamente tenendo conto della complessit√†, con funzione obiettivo ed errore su training set nullo.
- Minimizzando quella funzione obiettivo, come detto poc'anzi, massimizziamo il margine (leggere *nozioni utili*).
	- Con i vincoli ci assicuriamo che tutti i punti del training set siano correttamente classificati.

In realt√† noi risolveremo il problema in forma duale: $$\begin{align*}
\max\;\sum\limits_{i} \alpha_{i}- \sum\limits_{i,j} \alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{t}/2& \quad\forall\;i,j=1\ldots l\\
\alpha_{i}&\geq0\\
\sum\limits_{i}\alpha_{i}y_{i}&=0
\end{align*}$$ vogliamo trovare le soluzioni ottime $\alpha_{p}$ per $p\in \{1,\ldots,n\}$, cio√® i moltiplicatori di Lagrange, una volta trovati gli $\alpha$ possiamo calcolare $(w,b)$
$$\begin{align*}
w &= \sum\limits_{p} \alpha_{p}y_{p}x_{p}\\
b &= y_{k}-w^{T}x_{k}\quad\forall\;\alpha_{k}>0 \\
h(x) &= \text{sign}(w^{T}x + b) = \text{sign}\left( \sum\limits_{p=1}^{l}\alpha_{p}y_{p}x_{p}^{T}x + b \right)
\end{align*}$$ 
sapendo che $\alpha_{p}\neq0\implies x_{p}\text{ vettore supporto}$, la soluzione √® 
1. spesso sparsa
2. formulata solo in termini dei vettori di supporto

quindi l'iperpiano dipende soltanto dai vettori di supporto (anche se per trovare i valori di $\alpha$ servono comunque tutti i dati del training set).

Per questo motivo possiamo scrivere $h(x)$ in una forma diversa, in cui la sommatoria √® solo sui vettori di supporto (il cui insieme chiameremo $SV$). $$h(x) = \text{sign}\left( \sum\limits_{p\in SV}\alpha_{p}y_{p}x_{p}^{T}x + b \right)$$  ^5ab9d6
- Sono i vettori di supporto a determinare la soluzione.

![[iperpiano_supportVector.png]] 

In figura notiamo come l'eliminazione di due punti verdi casuali, che non siano *support vector* (quelli cerchiati), non cambia la retta. Risultato analogo se vengono presi in considerazione due punti bianchi (l'importante √® che non siano support vector).

```ad-theo
title: Regola del prodotto scalare 
Il prodotto scalare mostra quanto l'input e il $p$-esimo pattern si somiglino, tale valore √® pesato con $\alpha_{p}$ e $y_{p}$. $$h(x) = \text{sign}\left( \sum\limits_{p\in SV}\alpha_{p}y_{p}\quad \mathbf{x_{p}^{T}x} \quad + b \right)$$
```

### Soft Margin SVM
Richiedere che tutti i punti ricadano all'esterno del margine pu√≤ essere **troppo restrittivo**, potremmo pensare di ammettere alcuni errori sia per tollerare la presenza di dati rumorosi che per aumentare la dimensione del margine.
Per fare ci√≤ introduciamo delle **slack variables**.

![[slack_var1.png]]

In una situazione del genere, per separare le *"x"* e le *"o"* dovrei avere un margine piccolissimo (quello arancione), mentre eliminando i due punti pi√π restrittivi (quindi ammettendo errori) favorirei un margine molto pi√π ampio (cio√® quello blu).

```ad-def
title: Slack variable
Le slack variables non sono altro che variabili introdotte all'interno della SVM per permette un certo grado di errore di classificazione dei dati.

- Una slack variable $\xi_{i}$ per un certo punto di training √® la distanza tra il punto e il margine corrispondente.
	- Se il punto √® classificato come positivo, la variabile √® uguale alla distanza tra il punto e $w^{T}x + b = 1$.
	- Se il punto √® classificato come negativo, la variabile √® uguale alla distanza tra il punto e $w^{T}x + b = -1$.
```

![[slack_var2.png]]

Nell'esempio $\xi_{2}$  il punto √® classificato bene (sta nel lato giusto dell'iperpiano) ma si trova all'interno del margine. 
La situazione cambia per $\xi_{1}$, il punto √® mal classificato. Formalmente $$\begin{align*}
\xi_{1}:&\quad w^{T}x+b>1\\
\xi_{2}:&\quad 0<w^{T}x+b<1
\end{align*}$$

Nella formulazione primale del problema la situazione cambia (con l'introduzione delle slack variables). 
$$\begin{align*}
\min\;\frac{\lvert\lvert w \rvert\rvert^{2}}{2} &+ C\cdot\sum\limits_{p}\xi_{p}\\
(w^Tx_{p}+b)y_{p}&\geq 1-\xi_{p}\\\\

\xi_{p}&\geq0\quad\forall\;p=1\ldots l
\end{align*}$$ 

Con l'introduzione dell'iperparametro $C$, riusciamo a dare un peso all'errore e con ci√≤ a *guidare* il numero di errori ammessi. Spesso viene chiamata variabile di *penalty* (perch√© permette *misclassification* solo su determinati vincoli).
- Nella funzione obiettivo
	- Il primo termine √® quello per la regolarizzazione. $$\frac{\lvert\lvert w \rvert\rvert^{2}}{2}$$
	- Il secondo termine regola gli errori ammessi sul training $$C\cdot\sum\limits_{p}\xi_{p}$$
- Nei vincoli ogni pattern ha un proprio <u>margine d'errore</u> $$(w^Tx_{p}+b)y_{p}\geq \underline{1-\xi_{p}}$$

Abbiamo avuto un trade-off, da un lato abbiamo rilassato le ipotesi permettendo margine d'errore, ma dall'altro ora ci troviamo con un iperparametro da gestire (non abbiamo pi√π sicurezze su underfitting e overfitting).
- Se il valore √® troppo basso $\implies$ sono ammessi molti errori sul training set.
	- Troppi errori sul training set, VC-dim bassa (modello molto semplice) quindi **underfitting**.
- Se il valore √® troppo alto $\implies$ si tende ad avere zero errori sul training set.
	- Errore empirico nullo, VC-dim molto alta (modello molto complesso) quindi **overfitting**.
	- Abbiamo un margine piccolissimo.
	- e.g: $C\to\infty\implies\text{overfitting}$

### Parliamo del kernel
Rilassando anche la seconda ipotesi e considerando problemi non linearmente separabili.
- Possiamo mappare i punti dell'input space in uno spazio di **dimensioni maggiori** detto **feature space**, in cui i punti risultano linearmente separabili

![[feature_space.png]]

$$\phi: \mathbb R^{2}\to \mathbb R^{3}$$
$$\begin{pmatrix}x_{1}, x_{2}\end{pmatrix}^{T}\mapsto\begin{pmatrix}x_{1}^{2},\sqrt2 x_{1}x_{2},x_{2}^{2}\end{pmatrix}^{T}$$

Abbiamo visto questo approccio gi√† nella [[3. Modelli lineari#Linear Basis expansion|LBE]] in cui invece di avere come input $x$ avevamo $\phi(x)$ per gestire task non lineari **rispetto all'input**.

Come visto in precedenza, la LBE pu√≤ diventare costosa e portare all'overfitting se la dimensione di $\phi(x)$ cresce. 
Con l'approccio del **kernel** possiamo gestire implicitamente il feature space nel contesto in cui la complessit√† dei modelli :
- dipende dalla dimensione del margine.
- **NON DALLA DIMENSIONE DELL'INPUT**.

In questo modo anche all'aumentare della dimensione del feature space, la complessit√† del classificatore pu√≤ essere tenuta bassa.

Consideriamo la formula di $h(x)$ vista gi√† [[#^5ab9d6|qui]], usando per√≤ $\phi(x)$ al posto di $x$ 
$$h(x) =\text{sign}\left(\sum\limits_{p\in SV}\alpha_{p}y_{p}\phi(x_{p}^{T})\phi(x) + b\right)$$ non √® necessario calcolare il prodotto scalare $\phi(x_{p}^{T})\phi(x)$, infatti possiamo sostituirlo con una funzione kernel, rendendo la funzione $h(x)$ parametrizzabile $$h(x) =\text{sign}\left(\sum\limits_{p\in SV}\alpha_{p}y_{p}\;\text{K}(x_{p},x) + b\right)$$

```ad-def
title: Kernel
Un kernel $k:\mathbb R^{n}\times\mathbb R^{n}\to \mathbb R$ √® una funzione tale per cui esiste uno spazio Hilbertiano $X$ e una funzione $\phi:\mathbb R^{n}\to X$ (con $n$ molto grande), e si ha $$k(x_{i},x_{j}) = \phi(x_{i})^{T}\phi(x_{j})$$
```

Alcuni kernel popolari sono:
- **Lineare** $$k(x_{i},x_{j}) = x_{i}^{T}x_{j}$$
- **Polinomiale** ($\phi(x)$ ha dimensione polinomiale in $k$) $$k(x_i,x_j)=(1+x_{i}^{T}x_{j})^{k}$$
- **Radial Basis Function (RBF)** $$k(x_{i},x_{j})=\exp{\left( -\frac{\lvert\lvert x_{i}-x_{j} \rvert\rvert^2}{2\sigma^2} \right)}$$

L'RBF usa una gaussiana, notiamo che ha un iperparametro $\sigma$:
- Per $\sigma$ piccolo, il kernel ha un picco pi√π alto e stretto intorno a zero.
	- Pattern e input sono simili solo se sono molto vicini tra loro.
	- Il classificatore risponde con la classe assegnata al punto pi√π vicino.
	- Utile per discriminare punti del Training set ma pu√≤ causare overfitting.

Scegliamo quindi un trade off tra il parametro $C$, la funzione kernel $K$ e i suoi iperparametri. Dopo aver risolto questo problema di ottimizzazione troviamo i valori di $\alpha$ da usare nel modello finale $$h(x) =\text{sign}\left(\sum\limits_{p\in SV}\alpha_{p}y_{p}\;\text{K}(x_{p},x) + b\right)$$ in questo modo il costo computazionale cresce con $l$, cio√® con il numero di dati **e non con** $n$ (dimensione del feature space). Inoltre √® modulare! Possiamo usare lo stesso classificatore cambiando il kernel.

## Pratica
> Come nella vita, evitiamo di usare qualcosa con aspettative troppo ottimistiche ü´†

Con la SVM non si scappa dall'overfitting, pu√≤ sempre verificarsi se le componenti del modello
- C
- Kernel
- Parametri del Kernel

non sono scelte correttamente. Inoltre facendo crescere troppo $l$, aggiungendo variabili di input inutili alla classificazione, la complessit√† del modello cresce.

Infine bisogna usare le tecniche di validazione viste. La scelta delle componenti avviene sul **validation set** e il test anche in questo caso deve essere fatto su un insieme diverso di dati.

![[svm_practice.png]]

### Model selection
Per il model selection, di $C$ e $\gamma$ in RBF che dipende completamente da $\sigma$, prima si considera una ricerca a grana grossa per trovare gli intervalli migliori. Una volta trovati, si ricerca a grana pi√π fine all'interno di essi.

### Conclusioni
SVM √® un tool molto utile e buono a livello di performance **MA** 
> Quando hai un martello in mano, tutti i problemi diventano chiodi.

non √® sempre la scelta migliore. √à comunque valido perch√© combina LBE con un approccio che cerca di massimizzare il margine, quindi unisce flessibilit√† a controllo della complessit√†.