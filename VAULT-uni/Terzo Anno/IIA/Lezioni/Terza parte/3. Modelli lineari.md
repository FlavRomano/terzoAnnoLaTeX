```ad-note
title: Recap ingredienti
- **Dati** di allenamento
- **Spazio delle ipotesi** $H$
	- costituisce l‚Äôinsieme delle funzioni che possono essere realizzate dal sistema di   apprendimento;
	- si assume che la funzione da apprendere $f$ possa essere rappresentata da una ipotesi $h$ in $H$ (selezione di $h$ attraverso i dati di apprendimento)
	- o che almeno una ipotesi $h$ in $H$ sia simile a $f$ (approssimazione);
- **Algoritmo di apprendimento** *aka* Algoritmo di ricerca nello spazio delle ipotesi. e.g: adattamento dei parametri liberi del modello al task.

n.b: $H$ non pu√≤ coincidere con l'insieme di tutte le funzioni possibili e la ricerca essere esaustiva (Bias induttivo)
```

## Regressione
e.g Predire la temperatura di domani.

√à possibile fare learning con modelli semplici come le equazioni lineari.

La regressione √® il processo di stima di una funzione a valori reali sulla base di un insieme finito di campioni rumorosi.

e.g: Nota la coppia $$\langle x, f(x) + rumore \rangle$$ vogliamo trovare la $f$ per i dati della seguente tabella:

![[regression1.png]]

Vogliamo trovare le componenti del polinomio, cio√® $w_1$ e $w_0$, in maniera sistematica sulla base dei nostri dati. La retta $h(x) = 2x$ pu√≤ essere una buona ipotesi.

### Regressione lineare univariata
√à un caso semplice di regressione lineare. Si parte con un input $x$ e un output $y$. 
Assumeremo un modello $h_w(x)$ espresso da $$out = h(x) = w_{1}x + w_{0}$$
dove $w$ sono coefficienti a valori reali o **pesi**.

- Vogliamo fare fitting dei dati trovando la linea retta che passa da quest'ultimi.

```ad-example
title: Esempio di task e modello
Vogliamo trovare $h$ (modello lineare) che meglio si adatti ai dati (l'insieme dei dati $x$ e $y$ osservati).

![[aima_regressionEx.png]]

- Assumendo che una data variabile $y$ sia correlata, linearmente, a un'altra variabile $x$ (o da variabili $x_i$) attraverso $$y = w_{1}x + w_{0} + noise$$ dove $w_i$ sono parametri liberi e il rumore √® l'errore nella misurazione del target (errore con distribuzione normale).
- Costruiamo un modello, trovando i valori di $w$, per prevedere/stimare il prezzo $y$ dei punti per altri valori $x$ (non osservati). Quindi fare una **previsione**.

![[aima_regressionEx2.png]]
```

### Least mean square
In soldoni, dobbiamo trovare i valori dei parametri $w_i$ ($w_1$ e $w_0$ nel caso univariato) al fine di minimizzare l'errore di output del modello, fare quindi buon fitting.

Il problema √® che abbiamo uno spazio delle ipotesi infinito (valori continui di $w$), ci vengono in aiuto le scoperte di fine '700 di *Gauss* e *Legendre*.
- Definiamo una funzione di Loss (perdita o errore) e utilizziamo il **Least Mean Square**.
- Vogliamo imparare a trovare $w$ in modo da minimizzare l'errore (perdita empirica) per consentire miglio adattamento sui dati (insieme di training).

```ad-def
title: Least Mean Square
Dato un set di $l$ esempi di training $(x_{p}, y_{p})$ $$p = 1\ldots l$$ vogliamo trovare $h_{w}(x)$ nella forma $w_{1}x + w_{0}$ (quindi i valori di $w$) in modo tale da minimizzare la perdita attesa (expected loss) sui valori di training.

n.b: l'expected loss √® l'errore medio sui valori di training.

- Per la perdita utilizziamo il quadrato degli errori.
- Vogliamo trovare $w$ per minimizzare la somma dei quadrati residua 

$$Loss(h_{w}) = E[w] = \sum\limits_{p=1}^{l}(y_{p} - h_{w}(x_{p}))^{2} = \sum\limits_{p=1}^{l} (y_{p} - (w_{1}x_{p} + w_{0}))^2$$ dove $x_{p}$ √® il $p$-esimo input, $y_{p}$ √® il $p$-esimo output per $p$.
```

Un punto di minimo locale √® un punto stazionario, *ergo* l√¨ il gradiente √® nullo. Cercando i punti dove il gradiente √® nullo, troviamo il minimo della funzione che stiamo cercando. $$\frac{\partial}{\partial w_{i}} E[w] = 0 \quad i = 1,\ldots,dim\_input + 1$$
- Per la regressione lineare semplice (due parametri liberi) $$\frac{\partial}{\partial w_{0}}E[x] = 0\quad \frac{\partial}{\partial w_{1}}E[x] = 0$$ visto che la funzione √® convessa, non abbiamo minimi locali. $$w_{1}= \frac{\text{Cov(x,y)}}{\text{Var}(x)} \quad w_{0}=\overline y - w_{1}\overline x,\quad \text{con}\quad \overline y = \frac{1}{l}\sum\limits_{p\to l} y_{p}\quad \overline x = \frac{1}{l}\sum\limits_{p\to l}x_{p}$$ 

e.g: Calcolare il gradiente per ogni pattern $p$ $$\begin{align*}
\frac{\partial}{\partial w_{i}}E[x] &= \frac{\partial}{\partial w_{i}} (y - h_{w}(x))^{2} \\
&= 2\left(y - h_{w}(x)\right)\cdot \frac{\partial}{\partial w_{i}} (y - h_{w}(x))\\
\end{align*}$$ quindi $\dfrac{\partial}{\partial w_{0}} =-2(y-h_{w}(x))$ , $\dfrac{\partial}{\partial w_{1}} =-2(y-h_{w}(x)) \cdot x$

[[4. Ricerca locale#Ricerca locale in spazi continui| Repetita sugli spazi continui e il gradiente]]

### Discesa del gradiente (ricerca locale)
Ci√≤ che abbiamo detto fin'ora d√† un hint, costruire un algoritmo iterativo basato su $\dfrac{\partial}{\partial w_{i}} E[w]$

- Il gradiente fornisce la direzione di ascesa, *ergo* possiamo muoverci verso il minimo con una **discesa del gradiente**. Indicheremo con $\Delta w = -\nabla w = -\text{gradiente di }E[w]$ 

Parliamo di ricerca locale, iniziamo con un vettore di pesi iniziale. Verr√† modificato iterativamente per diminuire fino a minimizzare la funzione di errore (discesa rapida).

```ad-def
title: Learning rate (tasso di apprendimento)
Il tasso di apprendimento $\eta$ √® un iperparametro (noi abbiamo detto semplicemente "costante") che controlla i pesi del modello rispetto alla discesa del gradiente. $$w_{new} = w + \eta \cdot \Delta w$$ Definisce la velocit√† con cui il modello aggiorna i concetti appresi.
```

```ad-theo
title: Delta rule (error correction)
$\Delta w$ cresce rispetto alla distanza tra $y$ e $h$, la delta rule √® una regola che cambia i pesi $w$ proporzionalmente all'errore.
1. $$\text{target }y - \text{output }h = \text{errore} = 0 \implies \textbf{OK}, \text{ nessuna correzione}$$
2. $$\text{output} > \text{target} \implies y-h < 0 $$ in questo caso l'output √® troppo alto, dobbiamo abbassare la retta per avvicinarci al target.
	- Allora $\Delta w_{0}$ √® negativo, quindi dobbiamo ridurre $w_{0}$.
	- Se $(\text{input } x>0)$ allora $\Delta w_1$ √® negativo, quindi dobbiamo ridurre $w_1$
		- Altrimenti avremmo dovuto incrementare $w_1$.

Impariamo dai nostri errori ü•∫.
```

#### Conclusioni sul gradiente discendente
Il metodo di discesa del gradiente √® un approccio di ricerca locale semplice ed efficace per la soluzione dei Least Mean Squares, inoltre:
- Ci permette di cercare in uno spazio di ipotesi infinito.
- Pu√≤ essere sempre facilmente applicato  per $H$ continuo e loss differenziabile.
- Non √® il pi√π efficiente, si √® migliorato con gli anni (Metodi di Newton, gradiente coniugato).

### Estensione su $l$ patterns
Per $l$ patterns $(x_{p},y_{p})$ $$ \begin{align*}
\Delta w_{0} &= - \frac{\partial}{\partial w_{0}}E[w] = 2 \sum\limits_{p=1}^{l}(y_{p}- h_{w}(x_{p})) \\\\
\Delta w_{1} &= - \frac{\partial}{\partial w_{1}}E[w] = 2 \sum\limits_{p=1}^{l}(y_{p}- h_{w}(x_{p}))\cdot x_{p}
\end{align*} $$ dove $x_p$ √® il $p$-esimo input, $y_{p}$ √® il $p$-esimo output per $p$, $w$ parametro libero e $l$ il numero di esempi.

```ad-question
title: Quando aggiornare i pesi
Quando conviene aggiornare i pesi $w_{0}$ e $w_{1}$? Dopo un passo grande o dopo tanti piccoli passi?

![[aggiornamento_pesi.png]]

- Possiamo aggiornare $w$ dopo un *epoca* di $l$ esempi. Questa prassi √® nota come **Batch algorithm**. Nella figura √® in blu.
- Oppure aggiornare $w$ dopo ogni pattern $p$. Questo √® noto come **On-line algorithm** (discesa stocastica del gradiente). Nella figura √® in viola e verde.
	- Abbiamo un coefficiente di apprendimento $\eta$ minore ma pu√≤ essere il pi√π veloce.
```

### Input multidimensionali
Come notazione assumiamo $l = \#dati$ e $n = \#variabili$, avere come input pattern un vettore √® un caso molto comune: $$x = \begin{pmatrix}x_{1},x_{2},\ldots ,x_{n}\end{pmatrix}$$

e.g: 
1. Valutare (come punteggio) lo stato della scacchiera in una partita di dama in base al numero di pezzi bianchi/neri/re/catturati nel turno successivo: abbiamo 6 variabili. $w$ pesa tali caratteristiche della "scacchiera".
2. Valutare il prezzo della casa considerando anche il numero di stanze, l'et√† della casa, il rango del quartiere, ecc...

$X$ √® una matrice $l \times n$, con $l$ righe ed $n$ colonne

$$
\begin{array}{c c} 
& \begin{array}{c c c c} x_{1}\;\; & x_{2} & x_{i}\; & x_{n} \\ \end{array} \\
\begin{array}{c c c c}\text{Pat 1}\\ \ldots\\ \text{Pat }p \\ \ldots\end{array} &
\left(
\begin{array}{c c c c}
x_{1,1} & x_{1,2} &  & x_{1,n} \\
 \\
x_{p,1} & x_{p,2} & x_{p,i} & x_{p,n} \\
\\
\end{array}
\right)
\end{array}
$$

| Pattern  | $x_{1}$   | $x_{2}$   | $x_{i}$   | $x_{n}$   |
| -------- | --------- | --------- | --------- | --------- |
| Pat 1    | $x_{1,1}$ | $x_{1,2}$ |           | $x_{1,n}$ |
| $\ldots$ |           |           |           |           |
| Pat $p$  | $x_{p,1}$ | $x_{p,2}$ | $x_{p,i}$ | $x_{p,n}$ |
| $\ldots$ |           |           |           |           |

Ogni riga, generica $x$, pu√≤ essere un esempio (di input), pattern, instanza e cos√¨ via...

- Abbiamo $p = 1\ldots l$ e $i = 1\ldots n$
- Per il target tipicamente usiamo $y_{p}$ (stessa con $d$ oppure $t$).

$$w^{T}x + w_{0} = w_{0} + w_{1}x_{1}+ w_{2}x_{2}+\ldots+w_{n}x_{n} = w_{0} + \sum\limits_{i=1}^{n} w_{i}x_{i}$$ spesso la $T$ in $w^T$ viene omessa.

- $w_0$ √® nota come intercetta, *threshold*, *offset* o *bias* (non √® il bias induttivo).
- Spesso √® utile fissare il primo componente $x_{0} = 1$ per riscriverla cos√¨: $$\begin{align*}
w^{T}x &= x^{T}w\quad\text{(prodotto scalare)}\\
x^{T} &= \begin{pmatrix}1 & x_{1}& x_{2} & \ldots &x_{n} \end{pmatrix}\\
w^{T} &= \begin{pmatrix}w_{0}& w_{1}& w_{2} &\ldots &w_{n}\end{pmatrix}
\end{align*}$$

```ad-note
title: Dal punto di vista geometrico
$w^{T}x$ definisce un iperpiano, attraverso cui decidiamo le zone positive e negative

![[iperpiano.png]]

Dire che a sinistra √® positivo e a destra √® zero equivale a mettere un confine, dividendo in due il piano. 

![[iperfiano_classifier.png]]

Difatti la classificazione pu√≤ essere vista come una ripartizione del piano per i valori 0 e 1. 
```

### Classificazione per confini decisionali lineari
La classificazione pu√≤ essere vista come l'allocazione dello spazio di input in **regioni di decisione** (e.g: 0, 1). √à lineare per via di $w^{T}x$, a scalino e unitaria perch√© pu√≤ essere vista come una rete neurale con un neurone.

![[classificazione_lineare.png]]

```ad-def
title: LTU (by Chat-GPT)
√à un neurone artificiale di base utilizzato nelle reti neurali artificiali che impiega una funzione di attivazione a soglia per produrre un'uscita basata su valori di ingresso pesati.
- Una LTU riceve diversi valori di ingresso, li moltiplica per i pesi corrispondenti, li somma e poi applica una funzione di soglia alla somma. Se la somma supera il valore di soglia, il neurone si attiva e produce un'uscita pari a 1, altrimenti produce un'uscita pari a 0.
- La funzione di soglia utilizzata da una LTU pu√≤ essere una funzione a gradini, che produce un'uscita discontinua.
```

```ad-note
title: Threshold (bias $w_{0}$)
Dato un bias $w_{0}$, nell'LTU $$ h(x) = w^{T}x + w_{0}\geq 0 $$ √® equivalente a dire $$h(x) = w^{T}x \geq -w_{0}$$ con $-w_0$ come valore soglia. Questa forma enfatizza il ruolo del bias come valore di soglia per attivare l'uscita +1 del classificatore.
```

#### Esempio spam
Trovare una $h(mail) + 1$ per $spam$ e $-1$ per $not\text{-}spam$.
- Features (variabili) $\phi(mail) = words[0/1]$ $$e.g:\quad \phi_{k}(x) = contain(word_{k})$$
- $w$ √® il contributo in peso delle features in input alla predizione. $$e.g:\quad\text{peso positivo per }free\;money,\text{ negativo per }.unipi$$
- $x^{T}w$ √® la combinazione pesata.
- $h_{w}(x)$ fornisce la soglia per decidere se √® spam o meno. $$h_{w}(x) = \text{sign}\left( \sum\limits_{k}w_{k}\phi_{k}(x) \right) [> 0 \implies +1 = Spam]$$

### Il problema dell'apprendimento per classificatori lineari
Assumendo di usare i minimi quadrati, dati $l$ esempi di training, vogliamo trovare $w$ in modo tale da minimizzare la somma residua dei quadrati (questa √® LS; per la media, cio√® LMS,  bisogna dividere per $l$) $$E[w] = \sum\limits_{p=1}^{l}\left( y_{p}- {x_{p}^{T}w} \right)^{2} = \left\Vert y - Xw\right\Vert ^ 2$$ 

- In $E[w]$ non usiamo la forma $h(x)$ per la regressione, perch√© altrimenti l'errore diventerebbe non differenziabile.
- L'errore minimo:
	- Se $y_{p}= 1 \implies x_{p}^{T}w \to 1$
	- Se $y_{p}\in\{0,-1\}\implies x_{p}^{T}\to 0/-1$

In pi√π abbiamo l'algoritmo di discesa del gradiente (iterativo) che ci indica la direzione lungo la quale spostare (inclinare) l'iperpiano per la classificazione dei valori. 
$$\Delta w_{i}= - \frac{\partial}{\partial w_{i}} E[w] = \sum\limits_{p=1}^{l}(y_{p}- x_{p}^{T}w)\cdot x_{p,i}\quad i=0\ldots n$$

![[deltaW1.png]]

Se classificato male (perch√© il target √® $+1$) aggiorniamo la configurazione dei suoi parametri liberi in modo da non commettere lo stesso errore, per la regola di correzione dell'errore. Per fare ci√≤ incrementiamo i parametri (con $\eta$) fino al *delta* (quindi un valore positivo).

![[deltaW2.png]]

Ora √® corretto.

n.b: Per avere $\Delta w_i$ positivo occorre alzare i pesi üèãÔ∏è‚Äç‚ôÄÔ∏è.

```ad-def
title: Accuratezza
Media dei pattern correttamente classificati $$\text{Accuracy} = \frac{l - \text{num\_err}}{l}$$ $$\text{mean\_{}err} = \frac{1}{l}\underbrace{\sum\limits_{p=1}^{l}L\left( h(x_{p}),d_{p} \right)}_{\text{num\_{}err}}$$
$$L\left( h(x_{p}),d_{p} \right) = \begin{cases}
0\quad&h(x_{p}) = d_{p} \\
1\quad &o/w
\end{cases}$$
```

#### Congiunzioni con i modelli lineari
Consideriamo una $h$ s√¨ fatta
$$h(x) = \begin{cases}
1\quad &wx + w_{0}\geq0 \\
0\quad &o/w
\end{cases}$$ 
- con quattro variabili: $$x_{1}\land x_{2}\land x_{4}\iff y$$ $$\begin{align*}
1 x_{1}+ 1 x_{2}+ 0 x_{3} + 1 x_{4} &> 2\\
1 x_{1} + 1 x_{2} + 0 x_{3}+ 1 x_{4} &\geq 2.5
\end{align*}$$
- con due variabili: $$x_{1}\land x_{2}\iff y$$ $$\begin{align*}
1x_{1}+ 1x_{2} &> 1\\
1x_{1}+ 1x_{2}&\geq 1.5
\end{align*}$$ 
![[ExCongiunzioni_ModelloLineare.png]]

```ad-warning
title: Limitazioni
In geometria, due insiemi di punti in un grafico bidimensionale sono linearmente separabili quando: 
- I due inisiemi di punti possono essere completamente separati da una singola retta.
- Se si, allora possiamo fornire una soluzione esatta, altrimenti nada.

In generale, due gruppi sono linearmente separabili nello spazio $n$-dimensionale se possono essere separati da un iperpiano $(n-1)$-dimensionale.

```