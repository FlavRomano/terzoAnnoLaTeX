```ad-note
title: Recap ingredienti
- **Dati** di allenamento
- **Spazio delle ipotesi** $H$
	- costituisce lâ€™insieme delle funzioni che possono essere realizzate dal sistema di   apprendimento;
	- si assume che la funzione da apprendere $f$ possa essere rappresentata da una ipotesi $h$ in $H$ (selezione di $h$ attraverso i dati di apprendimento)
	- o che almeno una ipotesi $h$ in $H$ sia simile a $f$ (approssimazione);
- **Algoritmo di apprendimento** *aka* Algoritmo di ricerca nello spazio delle ipotesi. e.g: adattamento dei parametri liberi del modello al task.

n.b: $H$ non puÃ² coincidere con l'insieme di tutte le funzioni possibili e la ricerca essere esaustiva (Bias induttivo)
```

## Regressione
e.g Predire la temperatura di domani.

Ãˆ possibile fare learning con modelli semplici come le equazioni lineari.

La regressione Ã¨ il processo di stima di una funzione a valori reali sulla base di un insieme finito di campioni rumorosi.

e.g: Nota la coppia $$\langle x, f(x) + rumore \rangle$$ vogliamo trovare la $f$ per i dati della seguente tabella:

![[regression1.png]]

Vogliamo trovare le componenti del polinomio, cioÃ¨ $w_1$ e $w_0$, in maniera sistematica sulla base dei nostri dati. La retta $h(x) = 2x$ puÃ² essere una buona ipotesi.

### Regressione lineare univariata
Ãˆ un caso semplice di regressione lineare. Si parte con un input $x$ e un output $y$. 
Assumeremo un modello $h_w(x)$ espresso da $$out = h(x) = w_{1}x + w_{0}$$
dove $w$ sono coefficienti a valori reali o **pesi**.

- Vogliamo fare fitting dei dati trovando la linea retta che passa da quest'ultimi.

```ad-example
title: Esempio di task e modello
Vogliamo trovare $h$ (modello lineare) che meglio si adatti ai dati (l'insieme dei dati $x$ e $y$ osservati).

![[aima_regressionEx.png]]

- Assumendo che una data variabile $y$ sia correlata, linearmente, a un'altra variabile $x$ (o da variabili $x_i$) attraverso $$y = w_{1}x + w_{0} + noise$$ dove $w_i$ sono parametri liberi e il rumore Ã¨ l'errore nella misurazione del target (errore con distribuzione normale).
- Costruiamo un modello, trovando i valori di $w$, per prevedere/stimare il prezzo $y$ dei punti per altri valori $x$ (non osservati). Quindi fare una **previsione**.

![[aima_regressionEx2.png]]
```

### Least mean square
In soldoni, dobbiamo trovare i valori dei parametri $w_i$ ($w_1$ e $w_0$ nel caso univariato) al fine di minimizzare l'errore di output del modello, fare quindi buon fitting.

Il problema Ã¨ che abbiamo uno spazio delle ipotesi infinito (valori continui di $w$), ci vengono in aiuto le scoperte di fine '700 di *Gauss* e *Legendre*.
- Definiamo una funzione di Loss (perdita o errore) e utilizziamo il **Least Mean Square**.
- Vogliamo imparare a trovare $w$ in modo da minimizzare l'errore (perdita empirica) per consentire miglio adattamento sui dati (insieme di training).

```ad-def
title: Least Mean Square
Dato un set di $l$ esempi di training $(x_{p}, y_{p})$ $$p = 1\ldots l$$ vogliamo trovare $h_{w}(x)$ nella forma $w_{1}x + w_{0}$ (quindi i valori di $w$) in modo tale da minimizzare la perdita attesa (expected loss) sui valori di training.

n.b: l'expected loss Ã¨ l'errore medio sui valori di training.

- Per la perdita utilizziamo il quadrato degli errori.
- Vogliamo trovare $w$ per minimizzare la somma dei quadrati residua 

$$Loss(h_{w}) = E[w] = \sum\limits_{p=1}^{l}(y_{p} - h_{w}(x_{p}))^{2} = \sum\limits_{p=1}^{l} (y_{p} - (w_{1}x_{p} + w_{0}))^2$$ dove $x_{p}$ Ã¨ il $p$-esimo input, $y_{p}$ Ã¨ il $p$-esimo output per $p$.
```

Un punto di minimo locale Ã¨ un punto stazionario, *ergo* lÃ¬ il gradiente Ã¨ nullo. Cercando i punti dove il gradiente Ã¨ nullo, troviamo il minimo della funzione che stiamo cercando. $$\frac{\partial}{\partial w_{i}} E[w] = 0 \quad i = 1,\ldots,dim\_input + 1$$
- Per la regressione lineare semplice (due parametri liberi) $$\frac{\partial}{\partial w_{0}}E[x] = 0\quad \frac{\partial}{\partial w_{1}}E[x] = 0$$ visto che la funzione Ã¨ convessa, non abbiamo minimi locali. $$w_{1}= \frac{\text{Cov(x,y)}}{\text{Var}(x)} \quad w_{0}=\overline y - w_{1}\overline x,\quad \text{con}\quad \overline y = \frac{1}{l}\sum\limits_{p\to l} y_{p}\quad \overline x = \frac{1}{l}\sum\limits_{p\to l}x_{p}$$ 

e.g: Calcolare il gradiente per ogni pattern $p$ $$\begin{align*}
\frac{\partial}{\partial w_{i}}E[x] &= \frac{\partial}{\partial w_{i}} (y - h_{w}(x))^{2} \\
&= 2\left(y - h_{w}(x)\right)\cdot \frac{\partial}{\partial w_{i}} (y - h_{w}(x))\\
\end{align*}$$ quindi $\dfrac{\partial}{\partial w_{0}} =-2(y-h_{w}(x))$ , $\dfrac{\partial}{\partial w_{1}} =-2(y-h_{w}(x)) \cdot x$

[[4. Ricerca locale#Ricerca locale in spazi continui| Repetita sugli spazi continui e il gradiente]]

### Discesa del gradiente (ricerca locale)
CiÃ² che abbiamo detto fin'ora dÃ  un hint, costruire un algoritmo iterativo basato su $\dfrac{\partial}{\partial w_{i}} E[w]$

- Il gradiente fornisce la direzione di ascesa, *ergo* possiamo muoverci verso il minimo con una **discesa del gradiente**. Indicheremo con $\Delta w = -\nabla w = -\text{gradiente di }E[w]$ 

Parliamo di ricerca locale, iniziamo con un vettore di pesi iniziale. VerrÃ  modificato iterativamente per diminuire fino a minimizzare la funzione di errore (discesa rapida).

```ad-def
title: Learning rate (tasso di apprendimento)
Il tasso di apprendimento $\eta$ Ã¨ un iperparametro (noi abbiamo detto semplicemente "costante") che controlla i pesi del modello rispetto alla discesa del gradiente. $$w_{new} = w + \eta \cdot \Delta w$$ Definisce la velocitÃ  con cui il modello aggiorna i concetti appresi.
```

```ad-theo
title: Delta rule (error correction)
$\Delta w$ cresce rispetto alla distanza tra $y$ e $h$, la delta rule Ã¨ una regola che cambia i pesi $w$ proporzionalmente all'errore.
1. $$\text{target }y - \text{output }h = \text{errore} = 0 \implies \textbf{OK}, \text{ nessuna correzione}$$
2. $$\text{output} > \text{target} \implies y-h < 0 $$ in questo caso l'output Ã¨ troppo alto, dobbiamo abbassare la retta per avvicinarci al target.
	- Allora $\Delta w_{0}$ Ã¨ negativo, quindi dobbiamo ridurre $w_{0}$.
	- Se $(\text{input } x>0)$ allora $\Delta w_1$ Ã¨ negativo, quindi dobbiamo ridurre $w_1$
		- Altrimenti avremmo dovuto incrementare $w_1$.

Impariamo dai nostri errori ðŸ¥º.
```

#### Conclusioni sul gradiente discendente
Il metodo di discesa del gradiente Ã¨ un approccio di ricerca locale semplice ed efficace per la soluzione dei Least Mean Squares, inoltre:
- Ci permette di cercare in uno spazio di ipotesi infinito.
- PuÃ² essere sempre facilmente applicato  per $H$ continuo e loss differenziabile.
- Non Ã¨ il piÃ¹ efficiente, si Ã¨ migliorato con gli anni (Metodi di Newton, gradiente coniugato).

### Estensione su $l$ patterns
Per $l$ patterns $(x_{p},y_{p})$ $$ \begin{align*}
\Delta w_{0} &= - \frac{\partial}{\partial w_{0}}E[w] = 2 \sum\limits_{p=1}^{l}(y_{p}- h_{w}(x_{p})) \\\\
\Delta w_{1} &= - \frac{\partial}{\partial w_{1}}E[w] = 2 \sum\limits_{p=1}^{l}(y_{p}- h_{w}(x_{p}))\cdot x_{p}
\end{align*} $$ dove $x_p$ Ã¨ il $p$-esimo input, $y_{p}$ Ã¨ il $p$-esimo output per $p$, $w$ parametro libero e $l$ il numero di esempi.

```ad-question
title: Quando aggiornare i pesi
Quando conviene aggiornare i pesi $w_{0}$ e $w_{1}$? Dopo un passo grande o dopo tanti piccoli passi?

![[aggiornamento_pesi.png]]

- Possiamo aggiornare $w$ dopo un *epoca* di $l$ esempi. Questa prassi Ã¨ nota come **Batch algorithm**. Nella figura Ã¨ in blu.
- Oppure aggiornare $w$ dopo ogni pattern $p$. Questo Ã¨ noto come **On-line algorithm** (discesa stocastica del gradiente). Nella figura Ã¨ in viola e verde.
	- Abbiamo un coefficiente di apprendimento $\eta$ minore ma puÃ² essere il piÃ¹ veloce.
```

### Input multidimensionali
Come notazione assumiamo $l = \#dati$ e $n = \#variabili$, avere come input pattern un vettore Ã¨ un caso molto comune: $$x = \begin{pmatrix}x_{1},x_{2},\ldots ,x_{n}\end{pmatrix}$$

e.g: 
1. Valutare (come punteggio) lo stato della scacchiera in una partita di dama in base al numero di pezzi bianchi/neri/re/catturati nel turno successivo: abbiamo 6 variabili. $w$ pesa tali caratteristiche della "scacchiera".
2. Valutare il prezzo della casa considerando anche il numero di stanze, l'etÃ  della casa, il rango del quartiere, ecc...

$X$ Ã¨ una matrice $l \times n$, con $l$ righe ed $n$ colonne

$$
\begin{array}{c c} 
& \begin{array}{c c c c} x_{1}\;\; & x_{2} & x_{i}\; & x_{n} \\ \end{array} \\
\begin{array}{c c c c}\text{Pat 1}\\ \ldots\\ \text{Pat }p \\ \ldots\end{array} &
\left(
\begin{array}{c c c c}
x_{1,1} & x_{1,2} &  & x_{1,n} \\
 \\
x_{p,1} & x_{p,2} & x_{p,i} & x_{p,n} \\
\\
\end{array}
\right)
\end{array}
$$

| Pattern  | $x_{1}$   | $x_{2}$   | $x_{i}$   | $x_{n}$   |
| -------- | --------- | --------- | --------- | --------- |
| Pat 1    | $x_{1,1}$ | $x_{1,2}$ |           | $x_{1,n}$ |
| $\ldots$ |           |           |           |           |
| Pat $p$  | $x_{p,1}$ | $x_{p,2}$ | $x_{p,i}$ | $x_{p,n}$ |
| $\ldots$ |           |           |           |           |

Ogni riga, generica $x$, puÃ² essere un esempio (di input), pattern, instanza e cosÃ¬ via...

- Abbiamo $p = 1\ldots l$ e $i = 1\ldots n$
- Per il target tipicamente usiamo $y_{p}$ (stessa con $d$ oppure $t$).

$$w^{T}x + w_{0} = w_{0} + w_{1}x_{1}+ w_{2}x_{2}+\ldots+w_{n}x_{n} = w_{0} + \sum\limits_{i=1}^{n} w_{i}x_{i}$$ spesso la $T$ in $w^T$ viene omessa.

- $w_0$ Ã¨ nota come intercetta, *threshold*, *offset* o *bias* (non Ã¨ il bias induttivo).
- Spesso Ã¨ utile fissare il primo componente $x_{0} = 1$ per riscriverla cosÃ¬: $$\begin{align*}
w^{T}x &= x^{T}w\quad\text{(prodotto scalare)}\\
x^{T} &= \begin{pmatrix}1 & x_{1}& x_{2} & \ldots &x_{n} \end{pmatrix}\\
w^{T} &= \begin{pmatrix}w_{0}& w_{1}& w_{2} &\ldots &w_{n}\end{pmatrix}
\end{align*}$$

```ad-note
title: Dal punto di vista geometrico
$w^{T}x$ definisce un iperpiano, attraverso cui decidiamo le zone positive e negative

![[iperpiano.png]]

Dire che a sinistra Ã¨ positivo e a destra Ã¨ zero equivale a mettere un confine, dividendo in due il piano. 

![[iperfiano_classifier.png]]

Difatti la classificazione puÃ² essere vista come una ripartizione del piano per i valori 0 e 1. 
```

### Classificazione per confini decisionali lineari
La classificazione puÃ² essere vista come l'allocazione dello spazio di input in **regioni di decisione** (e.g: 0, 1). Ãˆ lineare per via di $w^{T}x$, a scalino e unitaria perchÃ© puÃ² essere vista come una rete neurale con un neurone.

![[classificazione_lineare.png]]

```ad-def
title: LTU (by Chat-GPT)
Ãˆ un neurone artificiale di base utilizzato nelle reti neurali artificiali che impiega una funzione di attivazione a soglia per produrre un'uscita basata su valori di ingresso pesati.
- Una LTU riceve diversi valori di ingresso, li moltiplica per i pesi corrispondenti, li somma e poi applica una funzione di soglia alla somma. Se la somma supera il valore di soglia, il neurone si attiva e produce un'uscita pari a 1, altrimenti produce un'uscita pari a 0.
- La funzione di soglia utilizzata da una LTU puÃ² essere una funzione a gradini, che produce un'uscita discontinua.
```

```ad-note
title: Threshold (bias $w_{0}$)
Dato un bias $w_{0}$, nell'LTU $$ h(x) = w^{T}x + w_{0}\geq 0 $$ Ã¨ equivalente a dire $$h(x) = w^{T}x \geq -w_{0}$$ con $-w_0$ come valore soglia. Questa forma enfatizza il ruolo del bias come valore di soglia per attivare l'uscita +1 del classificatore.
```

#### Esempio spam
Trovare una $h(mail) + 1$ per $spam$ e $-1$ per $not\text{-}spam$.
- Features (variabili) $\phi(mail) = words[0/1]$ $$e.g:\quad \phi_{k}(x) = contain(word_{k})$$
- $w$ Ã¨ il contributo in peso delle features in input alla predizione. $$e.g:\quad\text{peso positivo per }free\;money,\text{ negativo per }.unipi$$
- $x^{T}w$ Ã¨ la combinazione pesata.
- $h_{w}(x)$ fornisce la soglia per decidere se Ã¨ spam o meno. $$h_{w}(x) = \text{sign}\left( \sum\limits_{k}w_{k}\phi_{k}(x) \right) [> 0 \implies +1 = Spam]$$

### Il problema dell'apprendimento per classificatori lineari
Assumendo di usare i minimi quadrati, dati $l$ esempi di training, vogliamo trovare $w$ in modo tale da minimizzare la somma residua dei quadrati (questa Ã¨ LS; per la media, cioÃ¨ LMS,  bisogna dividere per $l$) $$E[w] = \sum\limits_{p=1}^{l}\left( y_{p}- {x_{p}^{T}w} \right)^{2} = \left\Vert y - Xw\right\Vert ^ 2$$ 

- In $E[w]$ non usiamo la forma $h(x)$ per la regressione, perchÃ© altrimenti l'errore diventerebbe non differenziabile.
- L'errore minimo:
	- Se $y_{p}= 1 \implies x_{p}^{T}w \to 1$
	- Se $y_{p}\in\{0,-1\}\implies x_{p}^{T}\to 0/-1$

In piÃ¹ abbiamo l'algoritmo di discesa del gradiente (iterativo) che ci indica la direzione lungo la quale spostare (inclinare) l'iperpiano per la classificazione dei valori. 
$$\Delta w_{i}= - \frac{\partial}{\partial w_{i}} E[w] = \sum\limits_{p=1}^{l}(y_{p}- x_{p}^{T}w)\cdot x_{p,i}\quad i=0\ldots n$$

![[deltaW1.png]]

Se classificato male (perchÃ© il target Ã¨ $+1$) aggiorniamo la configurazione dei suoi parametri liberi in modo da non commettere lo stesso errore, per la regola di correzione dell'errore. Per fare ciÃ² incrementiamo i parametri (con $\eta$) fino al *delta* (quindi un valore positivo).

![[deltaW2.png]]

Ora Ã¨ corretto.

n.b: Per avere $\Delta w_i$ positivo occorre alzare i pesi ðŸ‹ï¸â€â™€ï¸.

```ad-def
title: Accuratezza
Media dei pattern correttamente classificati $$\text{Accuracy} = \frac{l - \text{num\_err}}{l}$$ $$\text{mean\_{}err} = \frac{1}{l}\underbrace{\sum\limits_{p=1}^{l}L\left( h(x_{p}),d_{p} \right)}_{\text{num\_{}err}}$$
$$L\left( h(x_{p}),d_{p} \right) = \begin{cases}
0\quad&h(x_{p}) = d_{p} \\
1\quad &o/w
\end{cases}$$
```

#### Congiunzioni con i modelli lineari
Consideriamo una $h$ sÃ¬ fatta
$$h(x) = \begin{cases}
1\quad &wx + w_{0}\geq0 \\
0\quad &o/w
\end{cases}$$ 
- con quattro variabili: $$x_{1}\land x_{2}\land x_{4}\iff y$$ $$\begin{align*}
1 x_{1}+ 1 x_{2}+ 0 x_{3} + 1 x_{4} &> 2\\
1 x_{1} + 1 x_{2} + 0 x_{3}+ 1 x_{4} &\geq 2.5
\end{align*}$$
- con due variabili: $$x_{1}\land x_{2}\iff y$$ $$\begin{align*}
1x_{1}+ 1x_{2} &> 1\\
1x_{1}+ 1x_{2}&\geq 1.5
\end{align*}$$ 
![[ExCongiunzioni_ModelloLineare.png]]

```ad-warning
title: Limitazioni
In geometria, due insiemi di punti in un grafico bidimensionale sono linearmente separabili quando: 
- I due inisiemi di punti possono essere completamente separati da una singola retta.
- Se si, allora possiamo fornire una soluzione esatta, altrimenti nada.

In generale, due gruppi sono linearmente separabili nello spazio $n$-dimensionale se possono essere separati da un iperpiano $(n-1)$-dimensionale.

```