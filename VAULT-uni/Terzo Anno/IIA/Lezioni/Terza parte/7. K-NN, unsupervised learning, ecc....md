Table of contents

1. [[#K-Nearest Neighbors (supervised learning)|K-Nearest Neighbors (supervised learning)]]
	1. [[#K-Nearest Neighbors (supervised learning)#1-NN|1-NN]]
	1. [[#K-Nearest Neighbors (supervised learning)#K-NN|K-NN]]
	1. [[#K-Nearest Neighbors (supervised learning)#Limitazioni del K-NN|Limitazioni del K-NN]]
1. [[#K-means (unsupervised learning)|K-means (unsupervised learning)]]
	1. [[#K-means (unsupervised learning)#H space per il clustering|H space per il clustering]]
	1. [[#K-means (unsupervised learning)#Algoritmo batch (LBG)|Algoritmo batch (LBG)]]
	1. [[#K-means (unsupervised learning)#Limitazioni K-means|Limitazioni K-means]]


## K-Nearest Neighbors (supervised learning)
### 1-NN
```ad-def
title: 1-Nearest Neighbor
È uno dei primi classificatori della storia. Semplicemente salviamo in memoria i dati di training $$\langle x_{p},y_{p} \rangle\quad p=1\ldots l$$
Dato un input $x$, di dimensione $n$:
- Trova il più vicino degli esempi di training $x_i$, cioè il più simile all'input.
	- Trova l'indice $i$ tale che minimizzi $d(x,x_{i})$, cioè $$i(x) = \text{arg }\min_{p}  \; d(x,x_p)$$
	- e.g la distanza euclidea $$d(x,x_{p})= \lvert\lvert x - x_{p}\rvert\rvert$$
- Restituisce l'$i$-esimo target $y_{i}$.
```

![[1-nn.png]]

Il *1-NN* è chiaramente un classificatore molto semplice, i cui punti di forza sono:
- Flessibilità.
- Semplicità d'implementazione.

Tuttavia fa schifo perché:
- Fa zero errori sul training set
- Prono all'overfitting
- I decision boundary non sono lineari, ma piuttosto irregolari.
- Fa fitting del rumore (conseguenza del non commettere errori sul TR).

### K-NN
L'idea dietro al K-Nearest Neighbors è
> Piuttosto che fidarmi di un esempio solo, posso fidarmi di un vicinato di dati così da ammorbidire il risultato. Smussando rumore del training set.

![[5-nn_ex.png]] 

Nell'esempio in figura: 
- la 1-NN avrebbe classificato $x_q$ con $+$
- la 5-NN avrebbe classificato $x_q$ con $-$

```ad-def
title: K-nearest neighbors
È come prendere una stima locale $$\text{avg}_{k}(x) = \frac{1}{k}\sum\limits_{x_{i}\in N_{k}(x)} y_{i}$$ Dove
- $N_{k}(x)$ è un insieme contenente $k$ vicini di $x$ (i *più vicini* secondo una data distanza $d$).

Se c'è una chiara dominanza di una delle classi nelle vicinanze di un'esempio $x$, è probabile che $x$ stesso appartenga a quella classe. La regola di classificazione è quindi il voto a maggioranza tra i membri di $N_{k}(x)$. 

- In caso di classificazione
$$h(x) = \begin{cases}
1\quad&\text{avg}_{k}(x)>0.5 \\
0\quad& o/w
\end{cases}\quad\text{per }y_{i}=\{0,1\}$$
- In caso di regressione viene usata direttamente $avg_{k}$, visto che di suo restiuisce un valore nel continuo.
```

e.g: 15-nn
![[15-nn_ex.png]]

Molto meglio: 
- presenta errori di training.
- I decision boundary sono non lineari: sono ancora abbastanza, anche se meno, irregolari.
- Il decision boundary si adatta alle densità locali delle classi

```ad-def
title: Multiclass K-NN
Restituisce la classe più comune tra i $k$ vicini più prossimi (secondo una distanza).
```

In generale, il K-NN è un modello:
- Lazy
- Basato sulle istanze, non sul modello che si è costruito
- Distance-based, perché dipende fortemente dalla distanza che si utilizza.

Inoltre, nel modello lineare faccio training con un algoritmo e poi ottengo il risultato,
qui invece quando arriva il momento di rispondere deve fare una ricerca su milioni di pattern (costo del recupero delle informazioni).

### Limitazioni del K-NN
```ad-def
title: Curse of dimensionality
Succede quando $n$ è alta. Il volume dello spazio dell’insieme dei dati cresce esponenzialmente. Talmente velocemente che abbiamo bassa densità di sampling dei dati, quindi i primi vicini sono lontani.
```

```ad-def
title: Curse of noisy
Succede quando il target dipende da solo poche misurazioni (quelle correlate col target), quindi le restanti feature (poco rilevanti) potrebbero avvelenare il risultato. 
```

Quindi problemi a generalizzare.
## K-means (unsupervised learning)
```ad-def
title: Clustering
Il clustering è il processo di suddivisione di un insieme di dati in gruppi o cluster, in modo che i punti di dati all'interno di ciascun gruppo siano simili tra loro e dissimili da quelli di altri gruppi.
```

```ad-def
title: K-means
L'algoritmo K-means inizia selezionando casualmente $k$ centroidi, dove $k$ è il numero di cluster da formare. Quindi, ogni punto del set di dati viene assegnato al centroide più vicino in base alla distanza euclidea tra loro. Successivamente, il centroide di ciascun cluster viene aggiornato prendendo la media di tutti i punti dati in quel cluster. Questo processo viene ripetuto fino alla convergenza, cioè fino a quando i centroidi smettono di cambiare o viene raggiunto un determinato numero di iterazioni.

![[k-means.png]]

È un algoritmo semplice ed efficiente, in grado di gestire grandi insiemi di dati. Tuttavia, presenta alcune limitazioni, come la sensibilità ai centroidi iniziali e la necessità di specificare in anticipo il numero di cluster.
```

### H space per il clustering
Voglio piazzare i miei centroidi nel punto giusto, in maniera che intorno a loro possa costruire i miei clusters. Minimizzando questa loss (errore quadratico di distorsione) $$L \left( h(x_{p}) \right) = \lvert\lvert x_{p} - c(x_{p}) \rvert\rvert^{2}$$ $$c(x_{p})=\text{cluster di }x_{p}$$ si fa minimo errore di distorsione, quindi il centroide si trova nel punto giusto. Ogni pattern avrà il proprio centroide di riferimento e il migliore sarà quello con la minima loss.

```ad-note
title: Excursus sull'$H$ space
$H$ è lo spazio $n$-dimensionale in cui risiedono i dati, dove $n$ è il numero di features o variabili nel set di dati. Ogni punto di dati è rappresentato come un vettore nel feature space, dove ogni dimensione del vettore corrisponde a una particolare feature. L'obiettivo di k-means è quello di suddividere i punti dati in $k$ cluster in questo $H$ space.

L'algoritmo k-means opera nell'$H$ space aggiornando iterativamente i centroidi dei cluster e riassegnando i punti dati al centroide più vicino. La distanza euclidea è comunemente utilizzata per misurare la somiglianza tra i punti dati e i centroidi nel feature space.

Pertanto, l'$H$ space (o spazio delle caratteristiche/features) è un aspetto cruciale del clustering k-means, in quanto determina la misura di similarità e i risultati del clustering.
```

### Algoritmo batch (LBG)
1. Scegliere $k$ centroidi di cluster che coincidano con $k$ pattern scelti a caso o $k$ punti definiti a caso all'interno dell'ipervolume contenente l'insieme di pattern.
2. Assegnare ogni pattern al centroide di cluster più vicino (il vincitore).
3. Ricompilare i centroidi dei cluster (centroide geometrico, cioè media) utilizzando le attuali appartenenze ai cluster.
4. Se non viene rispettato un criterio di convergenza, si torna al punto 2. Ad esempio
	- nessuna (o minima) riassegnazione di pattern a nuovi centroidi di cluster
	- diminuzione minima dell'errore quadratico.

```ad-note
title: Dettagli
Una prima debolezza individuabile nella K-means è che $K$ è fissato.

- Nel punto (2) dell'algoritmo, per ogni vincitore vale $$i^{*}(x) = \text{arg }\min_{i}\;\lvert\lvert x-c_{i} \rvert\rvert^{2}$$ ora $x$ appartiene al cluster $i^{*}$
	- Ricordiamo che la distanza euclidea è $$\lvert\lvert x-c_{i} \rvert\rvert^{2} = \sum\limits_{j\to n} \left( x_{j}- c_{ij} \right)^{2}$$
- Nel punto (3) per ogni cluster $i$ la nuova media (centroide) è $$c_{i} = \frac{1}{\# \text{cluster}_{i}} \cdot \sum\limits_{j:\; x_{j}\in\text{cluster}} x_{j}$$ è una media vettoriale.
```

```ad-example
Mettiamo 3 centroidi a caso.
![[k-means_Ex1.png]]
1. Inizializziamo 3 prototipi ($k=3$).
![[k-means_Ex2.png]]
1. Coloro i pattern in base al centroide più vicino.
![[k-means_Ex3.png]]
1. Si sono formati nuovi gruppi, quindi ricalcolo i centroidi.
![[k-means_Ex4.png]]
--------
2. Ho i nuovi cluster, quindi ricoloro i pattern in funzione dei nuovi centroidi.
![[k-means_Ex5.png]]

E così via fino a convergere (spesso a un minimo locale).
```

### Limitazioni K-means
- Occorre fornire il numero di cluster da individuare, quindi si fa *try and error* per trovare il $K$ più adatto.
- Come ogni algoritmo di ricerca locale, è importante la partenza (potrebbe trovare minimi locali diversi per $L(x)$).

Analizzando la natura del cluster si possono fare molte migliorie, tuttavia il problema è non banale. Ad esempio utilizzando il concetto di *purity*, una metrica simile all'entropia. Qual'ora la classificazione dovesse fallire, non sarà colpa dell'algoritmo ma della natura intrinseca del cluster.
