Gli alberi decisionali rappresentano: 
- una disgiunzione di congiunzioni di vincoli 
- sul valore degli attributi.

![[decision-tree.png]]

$H$ è molto più largo a quanto visto con le sole regole congiuntive ([[2. Concept Learning#Algoritmo Find-S|Find S]])

## Induzione top-down degli alberi decisionali
Dato un insieme di esempi di training, gli algoritmi per la costruzione di alberi decisionali effettuano una ricerca nello spazio degli alberi decisionali.
- La costruzione dell'albero è top-down. 
- L'algoritmo esegue una ricerca greedy.

```ad-def
title: ID3
È un algoritmo utilizzato per costruire alberi decisionali a partire da un insieme di dati di training etichettati. 
- Viene selezionata la feature (attributo) migliore.
- Per ogni possibile valore di questa feature viene creato un nodo discendente. 
	- Gli esempi vengono suddivisi in base a questo valore.
- Il processo viene ripetuto per ogni nodo discendente, fino a quando tutti gli esempi sono classificati correttamente (o non ci sono più feature).

L'algoritmo ID3 presenta alcune limitazioni, come la tendenza a sovraadattare i dati di addestramento, la sensibilità al rumore e ai valori anomali nei dati e la difficoltà a gestire attributi continui. 
```

Come si seleziona il miglior attributo?
```ad-def
title: Entropia
In information theory, l'entropia è una misura della quantità di informazioni contenute in un messaggio o in un segnale. È definita come la quantità media di informazioni per simbolo in un messaggio.

L'entropia è utilizzata anche negli algoritmi di apprendimento automatico, dove viene utilizzata per misurare l'impurità o la casualità dei dati in un particolare nodo o cluster. 

L'obiettivo è minimizzare l'entropia trovando gli attributi (o features) più informative per la classificazione.
```

L'entropia misura l'impurità di un insieme di esempi. Essa dipende dalla distribuzione della variabile casuale $p$. Siano
- $S$ una collezione di esempi di training.
- $p_{+}$ la percentuale di esempi positivi in $S$.
- $p_{-}$ la percentuale di esempi negativi in $S$.

Assumendo $0\lg(0) = 0$
$$\text{Entropy}(S) = -p_{+}\lg(p_{+}) - p_{-}\lg(p_{-})$$

e.g:  $$\begin{align*}
\text{Entropy}([14+,0-]) &= - \frac{14}{14}\lg\left( \frac{14}{14} \right) - 0 \lg (0) = 0 \\
\text{Entropy}([9+,5-]) &= - \frac{9}{14}\lg\left( \frac{9}{14} \right) - \frac{5}{14} \lg\left( \frac{5}{14} \right) = 0.94\\
\text{Entropy}([7+,7-]) &= - \frac{7}{14}\lg\left( \frac{7}{14} \right) - \frac{7}{14}\lg\left( \frac{7}{14} \right) = \frac{1}{2}+ \frac{1}{2} = 1
\end{align*}$$ l'ultimo è un esempio di alta impurità, cioè bassa omogeneità.

![[entropia.png]]

```ad-def
title: Information gain
L'information gain è una misura della riduzione dell'entropia (cioè dell'incertezza) ottenuta con la suddivisione dei dati in base a un attributo (feature). 
- L'idea di base del information gain consiste nel selezionare l'attributo che fornisce le informazioni più utili per la classificazione in ogni nodo di un albero decisionale. 
- L'attributo con il maggior guadagno di informazioni viene scelto come criterio di suddivisione per quel nodo.

$$\text{Gain}(S,A) = \text{Entropy}(S) - \sum\limits_{v\in \text{Values}(A)} \frac{\lvert S_v \rvert}{\lvert S \rvert} \cdot \text{Entropy}(S_v)$$ dove $\text{Values}(A)$ è l'insieme dei possibili valori dell'attributo $A$, $Sv$ è un sottoinsieme di esempi di $S$ in cui $A$ ha valore $v$.

oss: Se riusciamo a ripartire in sottoinsiemi con entropia molto bassa, allora il gain sarà molto alto (perché riesco a discriminare molto).

e.g: $$[14+,0-]\quad[0+,7-]\quad\text{classificazioni clear}$$
```