Table of contents

1. [[#Induzione top-down degli alberi decisionali|Induzione top-down degli alberi decisionali]] 
	1. [[#Induzione top-down degli alberi decisionali#Algoritmo ID3|Algoritmo ID3]]
	1. [[#Induzione top-down degli alberi decisionali#Information gain|Information gain]]
	1. [[#Induzione top-down degli alberi decisionali#Gain Ratio|Gain Ratio]]
	1. [[#Induzione top-down degli alberi decisionali#Ricerca nello spazio delle ipotesi (alberi decisionali)|Ricerca nello spazio delle ipotesi (alberi decisionali)]]
	1. [[#Induzione top-down degli alberi decisionali#Bias (alberi decisionali)|Bias (alberi decisionali)]]
1. [[#Problemi degli alberi decisionali|Problemi degli alberi decisionali]]
	1. [[#Problemi degli alberi decisionali#Overfitting|Overfitting]]
		1. [[#Overfitting#Esempio dati rumorosi|Esempio dati rumorosi]]
	1. [[#Problemi degli alberi decisionali#Evitare l'overfitting|Evitare l'overfitting]]
	1. [[#Problemi degli alberi decisionali#Attributi con valori continui|Attributi con valori continui]]
	1. [[#Problemi degli alberi decisionali#Dati di training incompleti|Dati di training incompleti]]
	1. [[#Problemi degli alberi decisionali#Geometricamente|Geometricamente]]

![[playTennis.png]]

^bb74a8

Gli alberi decisionali rappresentano: 
- una disgiunzione di congiunzioni di vincoli 
- sul valore degli attributi.

![[decision-tree.png]]

$H$ √® molto pi√π largo a quanto visto con le sole regole congiuntive ([[2. Concept Learning#Algoritmo Find-S|Find S]])

## Induzione top-down degli alberi decisionali
Dato un insieme di esempi di training, gli algoritmi per la costruzione di alberi decisionali effettuano una ricerca nello spazio degli alberi decisionali.
- La costruzione dell'albero √® top-down. 
- L'algoritmo esegue una ricerca greedy.
### Algoritmo ID3

```ad-def
title: ID3
√à un algoritmo utilizzato per costruire alberi decisionali a partire da un insieme di dati di training etichettati. 
1. Viene selezionata la feature (attributo) migliore.
2. Per ogni possibile valore di questa feature viene creato un nodo discendente. 
	- Gli esempi vengono suddivisi in base a questo valore.
3. Il processo viene ripetuto per ogni nodo discendente, fino a quando tutti gli esempi sono classificati correttamente (o non ci sono pi√π feature).

L'algoritmo ID3 presenta alcune limitazioni, come la tendenza a sovraadattare i dati di addestramento, la sensibilit√† al rumore e ai valori anomali nei dati e la difficolt√† a gestire attributi continui. 
```

Come si seleziona il miglior attributo?
```ad-def
title: Entropia
In information theory, l'entropia √® una misura della quantit√† di informazioni contenute in un messaggio o in un segnale. √à definita come la quantit√† media di informazioni per simbolo in un messaggio.

L'entropia √® utilizzata per misurare l'impurit√† o la casualit√† dei dati in un particolare nodo o cluster. 

L'obiettivo √® minimizzare l'entropia trovando gli attributi (o features) pi√π informativi per la classificazione.
```

L'entropia misura l'impurit√† di un insieme di esempi. Essa dipende dalla distribuzione della variabile casuale $p$. Siano
- $S$ una collezione di esempi di training.
- $p_{+}$ la percentuale di esempi positivi in $S$.
- $p_{-}$ la percentuale di esempi negativi in $S$.

Assumendo $0\lg(0) = 0$
$$\text{Entropy}(S) = -p_{+}\lg(p_{+}) - p_{-}\lg(p_{-})$$

e.g:  $$\begin{align*}
\text{Entropy}([14+,0-]) &= - \frac{14}{14}\lg\left( \frac{14}{14} \right) - 0 \lg (0) = 0 \\
\text{Entropy}([9+,5-]) &= - \frac{9}{14}\lg\left( \frac{9}{14} \right) - \frac{5}{14} \lg\left( \frac{5}{14} \right) = 0.94\\
\text{Entropy}([7+,7-]) &= - \frac{7}{14}\lg\left( \frac{7}{14} \right) - \frac{7}{14}\lg\left( \frac{7}{14} \right) = \frac{1}{2}+ \frac{1}{2} = 1
\end{align*}$$ l'ultimo √® un esempio di alta impurit√†, cio√® bassa omogeneit√†.

![[entropia.png]]
### Information gain

```ad-def
title: Information gain
L'information gain √® una misura della riduzione dell'entropia (cio√® dell'incertezza) ottenuta con la suddivisione dei dati in base a un attributo (feature). 
- L'idea di base del information gain consiste nel selezionare l'attributo che fornisce le informazioni pi√π utili per la classificazione in ogni nodo di un albero decisionale. 
- L'attributo con il maggior guadagno di informazioni viene scelto come criterio di suddivisione per quel nodo.

$$\text{Gain}(S,A) = \text{Entropy}(S) - \sum\limits_{v\in \text{Values}(A)} \frac{\# S_v}{\# S} \cdot \text{Entropy}(S_v)$$ dove $\text{Values}(A)$ √® l'insieme dei possibili valori dell'attributo $A$, $Sv$ √® un sottoinsieme di esempi di $S$ in cui $A$ ha valore $v$.

oss: Se riusciamo a ripartire in sottoinsiemi con entropia molto bassa, allora il gain sar√† molto alto (perch√© riesco a discriminare molto). 

e.g: $$[14+,0-]\quad[0+,7-]\quad\text{classificazioni clear}$$
L'obiettivo √® quello di separare gli esempi in base al target, trovando l'attributo che discrimina gli esempi che appartengono a classi di obiettivi diverse.
```

```ad-example
title: Selezionare il miglior attributo
![[ex_BestAttr.png]]
- L'information gain viene calcolato per tutti gli attributi e quello con il gain pi√π alto viene selezionato come primo nodo decisionale (e.g outlook).
- I dati di training sono suddivisi per valori di outlook (e associati ai seguenti nodi).
- Se l'entropia √® diversa da zero in un insieme, l'albero continua a crescere l√¨ (con tali dati e attributi rimanenti).
```

```ad-warning
title: Problemi con l'information gain
L'information gain favorisce gli attributi con molti valori possibili. Consideriamo l'attributo Data nell'[[#^bb74a8|esempio Play Tennis]]. "Day" ha il massimo guadagno di informazioni: 

- Ogni giorno corrisponde a un sottoinsieme diverso che √® puro: $[1+, 0-]$ o $[0+, 1-]$, cio√® 0 entropia.
- Abbiamo perfetta separazione dei dati di training, ma pessima capacit√† di generalizzazione dato che "Day" parcellizza troppo (si fanno tanti sottoinsiemi piccolissimi ü•∫).
```

### Gain Ratio
$$\text{GainRatio}(S,A) \equiv \frac{\text{Gain(S,A)}}{\text{SplitInformation}(S,A)} = \frac{\text{Entropy}(S) - \displaystyle\sum\limits_{v\in \text{Values}(A)} \dfrac{\#S_v}{\# S} \cdot \text{Entropy}(S_v)}{\text{SplitInformation}(S,A)}$$ con $$\text{SplitInformation}(S,A) = - \sum\limits_{i=1}^{c}\frac{\# S_{i}}{\#S} \lg \frac{\#S_{i}}{\# S}$$
- $S_{i}$ sono gli insiemi ottenuti per partizione sul valore $v_{i}$ di $A$ fino a $c$.
- $\text{SplitInformation}$ misura l'entropia di $S$ rispetto ai valori di $A$.
	- Pi√π i dati sono dispersi in modo uniforme, pi√π √® alta.

La gain ratio penalizza gli attributi che dividono gli esempi in tante piccole classi, come "Day" che parcellizava troppo. 
- Sia $\# S = n$, Date divide gli esempi in $n$ sottoinsiemi. $$\begin{align*}
\text{SplitInformation}(S,Date) &= -\left[\left( \frac{1}{n}\lg \frac{1}{n} \right) + \ldots + \left(\frac{1}{n}\lg \frac{1}{n} \right)\right]\\
&= - \lg \frac{1}{n}\\
&= \lg n
\end{align*}$$ $$\text{SplitInformation}(S,Date) = \lg n \implies \text{SplitInformation}(S,Date) > 1 \iff n > 2$$ abbiamo ridotto il gain ratio.
- Invece scegliendo una $A$ che divide i dati equamente in due classi (Con $\frac{n/2}{2} = \frac{1}{2}$) $$\begin{align*}
\text{SplitInformation}(S,A) &= -\left[\left( \frac{1}{2}\lg \frac{1}{2} \right)+ \left( \frac{1}{2}\lg \frac{1}{2} \right)\right] \\
&= -\left(-\frac{1}{2}- \frac{1}{2}\right)\\
&=1 
\end{align*}$$ quindi nessuna riduzione in termini di gain ratio.

```ad-note
title: Aggiustare il gain ratio
Il valore di $\text{SplitInformation}(S,A)$ pu√≤ essere 0 o molto piccolo quando $\# S_{i} \approx \# S$ per qualche valore $i$. E cos√¨ facendo il gain ratio esplode.

e.g: Caso estremo con $\lvert S_{1} \rvert = 0$ e $\lvert S_{2}\rvert = n$.
$$\text{SplitInformation} = -\left( \frac{0}{n} \lg \frac{0}{n} + \frac{n}{n}\lg \frac{n}{n} \right) = 0$$ un attributo con lo stesso valore per tutti gli esempi fa schifo, non √® informativo.

Per mitigare questo effetto, usiamo la seguente euristica:
1. Calcoliamo il $\text{Gain}$ per ogni attributo.
2. Calcoliamo il $\text{GainRatio}$ solo degli attributi con $\text{Gain}$ sopra la media.
```

### Ricerca nello spazio delle ipotesi (alberi decisionali)
Ricerca ID3 (hill-climbing) attraverso lo spazio dei possibili alberi decisionali, dal pi√π semplice al pi√π complesso. Rispetto all'algoritmo [[2. Concept Learning#Concept Learning#Algoritmo Candidate Elimination|Candidate elimination]]
- Lo spazio delle ipotesi √® completo (rappresenta tutte le funzioni a valori discreti), mentre candidate elimination solo disgiunzioni.
- La ricerca mantiene solo una sola ipotesi corrente, mentre candidate elimination poteva mantenere tutto il [[2. Concept Learning#^ceea66|version space]].
- Non c'√® backtracking, non c'√® garanzia di ottimalit√† (ottimo locale).
- Pu√≤ terminare prima, accettando classi rumorose.

### Bias (alberi decisionali)
```ad-def
title: Bias induttivo
Qual √® il bias induttivo di questo tipo di apprendimento?
1. Si preferiscono alberi corti, per la ricerca incrementale, dal pi√π semplice al pi√π complesso.
	- Non √® sufficiente, √® un semplice bias di un algoritmo BF che genera tutti gli alberi di decisione e seleziona quello pi√π breve e consistente.
2. Si preferiscono alberi che posizionano vicino alla radice gli attributi ad alto information gain. Un **bias di costruzione**.

n.b: La restrizione non √® sopra lo spazio delle ipotesi, bens√¨ sulla strategia di ricerca.
```

```ad-def
title: Bias di ricerca (preferenza)
Riguarda la strategia di ricerca. In ID3 lo spazio delle ipotesi completo ma la strategia di ricerca incompleta (nel caso di prima la ricerca locale ha molti pi√π difetti).
```

```ad-def
title: Bias di linguaggio (restrizione)
Dovuti all'insieme delle ipotesi esprimibili o considerate. Candidate-Elmination ricerca in uno spazio di ipotesi incompleto, la strategia di ricerca √® completa.

A differenza di Candidate-Elmination, ID3 sposta il bias sulla ricerca.
```

Perch√© preferire il bias di ricerca rispetto a quello di linguaggio?
- Abbiamo pi√π potenza e flessibilit√†, controlliamo meglio l'overfitting.

```ad-note
title: Ricordando il rasoio di Occam
Se nei modelli lineari $$\text{rasoio} = \text{regolare il polinomio}$$ per controllare la complessit√†, negli alberi decisionali $$\text{rasoio} = \text{scegliere l'albero pi√π corto}$$
```

## Problemi degli alberi decisionali
### Overfitting
Costruire alberi che si adattano troppo agli esempi di training, pu√≤ portare all'overfitting.

Considerando l'errore dell'ipotesi $h$ su:
- Esempi di training $$error_{D}(h)\quad\text{(errore empirico)}$$
- Intera distribuzione $X$ di dati $$error_{X}(h)\quad\text{(errore atteso)}$$

L'ipotesi $h$ *overfitta* i dati di training se esiste un'ipotesi alternativa $h'\in H$ tale che $$error_{D}(h) < error_{D}(h'), \; error_{X}(h')< error_{X}(h)$$ cio√®, un'ipotesi per cui nel training dei dati ho accuratezza sul training set, ma nella generalizzazione si comporta meglio $h'$.

Gli approcci flessibili possono facilmente andare incontro a overfitting.

Sia $\text{Accuracy} = \% \text{di dati correttamente fittati} = 1 - \%\text{errore}$

![[overfitting_dt.png]]

Per $h=70$ abbiamo $$error_{D}(h) = 1 - 0.85 = 0.15 \quad error_{X}(h) = 1 - 0.7 = 0.3$$  mentre per $h' = 15$ abbiamo $$error_{D}(h') = 1 - 0.77 = 0.23 \quad error_{X}(h') = 1 - 0.75 = 0.25$$
- $error_{D}(h) < error_{D}(h')$ ? $0.15<0.23$, SI
- $error_{X}(h') < error_{X}(h)$ ? $0.25<0.3$, SI

$\implies h$ fa overfitting.

#### Esempio dati rumorosi

![[ex_datiRumorosi.png]]

Potrei fare un altro splitting e decidere (secondo un altro attributo) in modo che venga no e gli altri si. Andrei a fare fitting anche con questo esempio rumoroso, quindi sarebbe perfetto sui dati di training ma generalizza malissimo.

### Evitare l'overfitting
> L'overfitting non si evita, al massimo si pu√≤ attenuare.

√à un fenomeno tipico dei modelli flessibili, non si pu√≤ scampare all'overfitting. Tuttavia abbiamo due strategie per attenuarne gli effetti.
1. Interrompere la crescita dell'albero in anticipo, prima della classificazione perfetta (early stop).
2. Consentire all'albero l'overfitting sui dati, e dopo effettuare post-prune dell'albero (poto dopo).

Come valutare l'effetto?
- Dividere l'insieme di training in due parti (training set e validation set).
- Utilizzare il validation set per valutare l'utilit√† di (1) e (2).

```ad-def
title: Reduced-error pruning
Ogni nodo √® un candidato per il pruning.
- La potatura consiste nel rimuovere un sottoalbero radicato in un nodo, il nodo diventa una foglia e gli viene assegnata la classificazione pi√π comune.
- I nodi vengono rimossi solo se l'albero risultante non peggiore le prestazioni sul **validation set**.
- I nodi vengono potati iterativamente, a ogni iterazione viene potato il nodo la cui rimozione aumenta maggiormente l'accuratezza sul validation set.
- La potatura si interrompe quando fare pruning non aumenta pi√π l'accuratezza.

e.g: 
![[ex_REPruning.png]]
```

```ad-theo
title: Rule post-pruning
1. Crea l'albero decisionale dall'insieme di training.
2. Convertire l'albero in un insieme equivalente di regole dove:
	- Ogni percorso corrisponde a una regola.
	- Ogni nodo lungo un percorso corrisponde a una **pre-condizione**.
	- Ogni nodo foglia classifica una **post-condizione**.
	e.g: $(Outlook = Sunny) \land (Humidity = High) \implies (PlayTennis=No)$
3. Fare pruning (generalizzare) ogni regola rimuovendo le precondizioni, la cui rimozione migliora l'accuratezza su:
	- Validation set.
	- Training set con una misura pessimistica.
4. Ordinare le regole in ordine di accuratezza e considerarle in sequenza quando si classificano nuove istanze.
```

- Ogni percorso distinto produce una **regola diversa**, la rimozione di una condizione pu√≤ essere basata su un criterio locale.
- La potatura delle precondizioni √® specifica della regola (percorso), la potatura dei nodi √® globale e riguarda tutte le regole (sottoalbero).

### Attributi con valori continui
Fin'ora abbiamo visto valori discreti sia per gli attributi che per l'esito di essi. 
Dato un attributo a valori continui $A$, dinamicamente viene creato un nuovo attributo $A_{c}$ $$A_{c}= \begin{cases}
True\quad &A<c \\
False\quad &o/w
\end{cases}$$ come determiniamo la soglia per $c$?

e.g: $Temperature$ nell'esempio di $PlayTennis$
- Ordiniamo gli esempi per $Temperature$ ascendente: 
![[playTennis_table.png]]
- Determiniamo le soglie candidate calcolando la media dei valori consecutivi in cui **si verifica un cambiamento nella classificazione** $$\frac{48+60}{2}=54,\;\frac{80+90}{2}=85$$
- Valutare le soglie candidate (attributi) in base all'information gain. Il migliore √® $$Temperature > 54$$ poi, il nuovo attributo compete con gli altri.

### Dati di training incompleti
Come si fa quando manca il valore di un attributo?
e.g: Risultato dell'esame del sangue in un problema di diagnosi medica.

- Utilizzare altri esempi per indovinare l'attributo, all'interno dei dati di addestramento di un dato nodo.
- Imputation:
	1. Pi√π comune, assegnare il valore pi√π comune tra tutti gli esempi di training del nodo o della stessa classe.
	2. Assegnare una probabilit√† $p_{i}$ a ciascun valore $v_{i}$, in base alle frequenze, e assegnare i valori all'attributo mancante, in base a questa distribuzione di probabilit√† (aggiungendo altri esempi ponderati per la probabilit√†).
	3. Classificare il nuovo esempio nello stesso modo (ponderazione) e scegliere la classificazione pi√π probabile.

```ad-note
title: Gestione di attributi con costi diversi
Gli attributi dell'istanza possono avere un costo associato, preferiremmo alberi decisionali che utilizzano attributi a basso costo. Per farlo, ID3 pu√≤ essere modificato $$\dfrac{\text{Gain}^{2}(S,A)}{\text{Cost}(A)}$$
```

### Geometricamente
Un albero decisionale pu√≤ produrre decision boundaries. Gli alberi decisionali dividono lo spazio di input in rettangoli paralleli agli assi ed etichettano ogni rettangolo con una delle $K$ classi (foglia dell'albero).

![[ex_geometricalView.png]]