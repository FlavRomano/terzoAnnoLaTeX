Table of contents

1. [[#Validation|Validation]]
	1. [[#Validation#Obiettivi della validation|Obiettivi della validation]]
	1. [[#Validation#Hold out|Hold out]]
		1. [[#Hold out#Meta-algoritmo|Meta-algoritmo]]
	1. [[#Validation#Grid search|Grid search]]
	1. [[#Validation#Separare TR, VL e TS|Separare TR, VL e TS]]
	1. [[#Validation#K-fold cross validation (CV)|K-fold cross validation (CV)]]
	1. [[#Validation#Comportamento tipico durante l'apprendimento|Comportamento tipico durante l'apprendimento]]
1. [[#Statistical Learning Theory (SLT)|Statistical Learning Theory (SLT)]]
	1. [[#Statistical Learning Theory (SLT)#Vapnik-Chervonenkis dim|Vapnik-Chervonenkis dim]]


## Validation
Ogni volta che si aumenta l'accuracy c'è il rischio di andare in overfitting. Bisogna quindi **valutare** la capacità di **generalizzazione dell'ipotesi**. È proprio questo che fa la validazione.

```ad-note
title: Un buon modello di ML
L'apprendimento consiste nella ricerca di una buona funzione in uno spazio.
> Qualunque $h$ che approssima bene $f$ sul training set approssimerà bene anche su istanze mai viste prima.
> -Ipotesi di apprendimento induttivo

Tuttavia nel mondo reale non è assolutamente possibile prenderla come regola generale, perché  avere troppa aderenza con il training porta all'overfitting.

Abbiamo diverse **misure**:
- Per la **classificazione**: l'errore quadratico medio per la loss, l'accuracy...
- Per la **regressione**: l'errore quadratico medio, l'errore quadratico assoluto...

In generale $$\text{errore alto} \iff \text{accuratezza bassa}$$ sia per training set che per i test.
- Basso fitting $\iff$ alto errore training.
- Bassa generalizzazione $\iff$ alto errore di test.
```

### Obiettivi della validation
Vi sono due obiettivi: Model selection e Model assessment.

```ad-def
title: Model Selection
Stima delle performance (prestazioni) di modelli differenti di apprendimento per scegliere il migliore da generalizzare. Restituisce un modello (include la ricerca dei parametri migliori per il modello).
```

```ad-def
title: Model Assessment
Scelto il modello finale, occorre stimare il rischio (errore) di predizione su dati di test (nuovi). Misuro la qualità del modello finale. Restituisce una stima:
> e.g: *Predice all'* $85\%$ *di accuratezza*
```

- Se teniamo separati questi due obiettivi, con dataset separati per farlo, tutto va per il meglio.
	- Ma non è sempre così semplice.

### Hold out

```ad-def
title: Hold out
Hold-out è quando dividi il set di dati in un set di training e uno per il test. 
- Il set di training è ciò su cui viene addestrato il modello
- il set di test viene utilizzato per vedere come si comporta bene quel modello su dati non noti. 

Una suddivisione comune quando si utilizza il metodo di Hold-out consiste nell'utilizzare l'80% dei dati per il training e il restante 20% dei dati per i test.
```

Se il data set ha una dimensione sufficiente $$e.g:\quad50\%\text{ TR},\;25\%\;\text{VL},\;25\%\;\text{TS}\quad\text{disgiunti}$$

![[hold_out.png]]

Ricorda, insiemi disgiunti:
- Il **Training set** TR è usato per il training.
- Il **Validation set** VL è usato per selezionare il miglior modello (tra tutti e/o le diverse configurazioni di iperparametri) 
	- Questa parte è nota come **model selection**.
	- L'unione di TR+VL viene chiamata *"Development set"*, perché calibrando il modello sui vari parametri ottengo il migliore possibile.
- Il **Test set** TS è usato per stimare quanto il modello sia in grado di generalizzare.
	- Questa parte è nota come **model assessment**.

n.b: la stima fatta durante il model assessment non è valida anche per la valutazione del rischio.

n.b: I risultati del TS non possono essere utilizzati durante la fase di model selection.

```ad-theo
title: Gold rule
Mantenere separazione tra goal e l'uso di insiemi separati.
- TR per il training
- VL per model selection
- TS per stima del rischio.

![[schema_trvlts.png]]
```

#### Meta-algoritmo
Siano $w$ parametri liberi e $\lambda$ iperparametri.
In generale un algoritmo di valutazione dovrebbe:
- Separare TR, VL e TS.
- Cercare la miglior $h_{w,\lambda}()$ cambiando gli iperparametri $\lambda$ del modello (e.g: ordine del polinomio per regressione lineare).
- Per ogni diverso valore di $\lambda$ (grid search):
	- Si cerca la miglior $h_{w,\lambda}()$ tale che minimizzi errore quadratico medio/loss empirica (faccia fitting con il TR set) trovando i migliori parametri liberi $w$.
- [Opzionalmente] È possibile fare fit con $h_{w,\lambda}(x)$ su TR+VL (development set) con la miglior $\lambda$.
- Valutare sul TS $h_{w,\lambda}(x)$ trovata.

### Grid search
Serve per trovare il miglior valore per un iperparametro.
La ricerca può essere vista come un `for` loop su una griglia (matrice) di valori candidati.
- Per ogni modello trainato $h_{w,\lambda}$ calcola l'accuratezza sul VL set.
- Prende l'$h_{w,\lambda}$ con minor errore quadratico medio o massima accurattezza.

**IMPORTANTE:** per valutare la qualità di un modello fare affidamento sempre al VALIDATION SET.
e.g: 

| $\lambda$ | TR  | VL  | TS  |
| --------- | --- | --- | --- |
| 0.5       | 75  | 70  | 70  |
| 0.1       | 80  | 75  | 70  |
| 0.01      | 90  | 70  | 72  |

-   In che ordine si usano le porzioni di dati per calcolare i valori in tabella?
	- Ping Pong di TR+VL e poi TS.
- Quale modello (ossia lambda) si sceglie?
	- Si sceglie sul validation $\implies \lambda=0.1$
- Che fenomeni si osservano?
	- Non ho voglia.

### Separare TR, VL e TS
Dati 20 esempi e 1000 variabili di input random (con target binario), immaginiamo di avere un modello con una sola feature che indovina al 99\% sul dataset e poi su qualsiasi spli successivo in TR, VL e TS.

- Va bene avere un modello con 99\% di accuratezza?
	- Assolutamente no, perché in primi non è un buon indice per la stima del rischio. Come abbiamo detto prima la model assessment non c'entra nulla con il risk assessment.
	- I dati di TR o VL non devono essere usati per scopi di test, c'è un motivo se abbiamo insiemi separati. Usare tutto il data set per model selection lede la correttezza della stima (**risultati biased**, anche noto come **Feature Selection bias**).

In pratica si è usato il test set all'inizio (spesso anche implicitamente).
oss. Il TS deve essere separato prima di effettuare qualsiasi tipo di model selection/assessment.

- Un test set esterno fornisce invece la stima corretta del 50\%, sicuramente non avvelenata da errori nell'organizzazione degli insiemi.

![[counterexample_table.png]]

Il 26esimo input aderisce al 100\% al target effettivo, com'è possibile notare dai test il 26esimo da un risultato illusorio riguardo l'accuratezza della previsione. Basti osservare che già il 27esimo e il 28esimo abbassano l'accuratezza.

### K-fold cross validation (CV)

```ad-def
title: K-fold cv
K-fold cross validation è un modo per migliorare il metodo di holdout. Il set di dati viene suddiviso in $k$ sottoinsiemi e il metodo di holdout viene ripetuto $k$ volte. 

Ogni volta, uno dei $k$ sottoinsiemi viene utilizzato come set di test e gli altri $k-1$ sottoinsiemi vengono messi insieme per formare un set di training. Quindi si calcola l'errore medio su tutte le $k$ prove. 

Il vantaggio di questo metodo è che non ha molta importanza la suddivisione dei dati. Ogni dato viene inserito in un set di test esattamente una volta e in un set di addestramento $k-1$ volte. In più si può scegliere in modo indipendente la dimensione di ciascun set di test e il numero di prove su cui effettuare la media.

- La varianza della stima risultante si riduce con l'aumento di $k$. 
```

È un modo furbo per scegliere il modello sulla base dei dati di cui disponiamo.
- Separiamo l'insieme di dati $D$ in $k$ sottoinsiemi mutuamente esclusivi $$D_{1},D_{2},\ldots,D_{k}$$
- Addestriamo l'algoritmo di training su $D/D_{i}$ e testiamolo su $D_{i}$.
- Prendiamo sulla diagonale $D_{i}$ i risultati e facciamone una media.

n.b: Questa tecnica può essere usata sia per il validation set o per il test set.

```ad-note
title: La scelta del $k$
Il $k$ decide quante fold verranno eseguite sull'insieme di dati di partenza (che sia esso di training/validation o testing).
- $k$ piccolo $\implies$ molto dispendioso.
- $k$ grande $\implies$ poco preciso.
```

```ad-example
title:e.g: Model selection and assessment con K-fold CV
Ovviamente il primo passo è dividere i dati in TR e Test set (golden rule).
- **Model selection**. Usiamo K-fold CV sull'insieme TR, in questo modo otteniamo nuovi insiemi TR e VL per ogni split. Un volta fatto ciò possiamo applicare una grid-search per cercare i migliori iperparametri possibili.
- Training del modello finale sul TR set.
- **Model assessment**. Valutiamo il modello trovato attraverso il Test set.
```

### Comportamento tipico durante l'apprendimento
![[typical_learning.png]]

e.g: Modello molto semplice. Underfitting.
![[typical_learning2.png]]

![[typical_learning3.png]]

e.g: Modello troppo complesso. Overfitting.
![[typical_learning4.png]]

![[typical_learning5.png]]

Possiamo notare come la complessità del modello sia importante... ma il numero di dati a disposizione non è da meno, entrambi sono due fattori determinanti. Il rapporto fra i due è importante.

![[small_dataSetEx.png]]

![[big_dataSetEx.png]]

con $l=100$ ho un'approssimazione eccellente della funzione target.

## Statistical Learning Theory (SLT)
La SLT è una teoria generale che lega quanto visto fin'ora riguardo:
1. Il ruolo della complessità del modello.
2. Il ruolo del numero dei dati.

```ad-theo
title: Impostazione formale
Vogliamo approssimare la funzione $f(\vec x)$, $d$ è il vettore target (con $d = f + noise$), minimizzando la funzione di rischio $$R = \int L(d, h(\vec x))\;dP(\vec x,d)$$ che rappresenta l'errore su tutti i dati.

Conoscendo
- $d$ dato dal supervisore e la distribuzione in probabilità $P(\vec x, d)$.
- una funzione di loss $$e.g:\quad L(h(\vec x), d) = (d - h(\vec x))^2$$

Quindi il problema diventa cercare $h$ nello spazio delle ipotesi $H$ minimizzando $R$.
- Bisogna fare attenzione, abbiamo un dataset finito $TR = (\vec x_{p}, d_{p})$ per $p = 1\ldots l$
- Per cercare $h$ occorre minimizzare il **rischio empirico** (errore di training $E$), cercando i migliori valori per i parametri liberi del modello. $$R_{emp}= \frac{1}{l}\sum\limits_{p=1}^{l}(d_{p}- h(\vec x_{p}))^{2}$$ questo è noto come **principio induttivo di minimizzazione del rischio empirico** (perché fatto sui dati di training).

oss: se cerco di minimizzare l'errore sui dati in training, $R_{emp}$ è una buona approssimazione di $R$?
```

### Vapnik-Chervonenkis dim

```ad-def
title: VC-dim
La VC-dimension è una misura della complessità di $H$, dove per complessità intendiamo la sua flessibilità riguardo al fitting dei dati.
```

```ad-def
title: VC-bounds
Mantiene con probabilità $1-\delta$ che il rischio garantito $$R \leq R_{emp} + \underbrace{\varepsilon \left(\frac{1}{l}, VC, \frac{1}{\delta}\right)}_{VC-confidence}$$
- $\varepsilon$ è una funzione che cresce proporzionalmente a $VC$ e decresce con (valori alti) $l$ e $\delta$.
- Sappiamo che $R_{emp}$ decresce utilizzando modelli complessi $\implies$ con $VC-dim$ alta.
- $\delta$ è la confidenza, governa la probabilità che mantiene il bound (e.g: $\delta = 0.01 \implies \mathbb P = 0.99$).
```

In generale:
- Con molti dati ($l$ alta) $\implies$ si abbassa la VC-confidence e il bound (la disequazione) si avvicina molto a $R$.
- Un **modello molto semplice**, quindi con una bassa VC-dim, non è sufficiente a causa dell'alto $R_{emp}$ (**underfitting**).
- Un **modello complesso**, quindi con alta VC-dim, abbassa $R_{emp}$ ma VC-confidence (e di conseguenza $R$) potrebbe aumentare (**overfitting**).

Vogliamo fare **minimizzazione strutturale del rischio** minimizzando il bound.

![[VC-plot.png]]
