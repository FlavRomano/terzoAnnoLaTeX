Table of contents

1. [[#Agente|Agente]]
1. [[#Agente Razionale|Agente Razionale]]
1. [[#Ambiente|Ambiente]]
	1. [[#Ambiente#Proprietà degli ambienti|Proprietà degli ambienti]]
	1. [[#Ambiente#Simulatore di ambienti|Simulatore di ambienti]]
1. [[#Struttura degli agenti|Struttura degli agenti]]
	1. [[#Struttura degli agenti#Programma agente|Programma agente]]
	1. [[#Struttura degli agenti#Agenti reattivi semplici|Agenti reattivi semplici]]
	1. [[#Struttura degli agenti#Agenti basati su modello|Agenti basati su modello]]
	1. [[#Struttura degli agenti#Agenti con obiettivo|Agenti con obiettivo]]
	1. [[#Struttura degli agenti#Agenti con valutazione di utilità|Agenti con valutazione di utilità]]
	1. [[#Struttura degli agenti#Agenti che apprendono|Agenti che apprendono]]
	1. [[#Struttura degli agenti#Tipi di rappresentazione|Tipi di rappresentazione]]

## Agente
```ad-def
title: Agente
Un agente è un'entità che percepisce dal suo ambiente attraverso dei sensori e agisce su tale ambiente attraverso degli attuatori (sono situati, hanno abilità sociale e intenzionale).
- Usiamo il termine **percezione** per riferirci agli input percettivi dell'agente in un dato istante. 
- La sequenza di percezioni di un agente è la storia completa di tutto ciò che l'agente ha percepito. 
- In generale, la scelta dell'azione di un agente in un dato istante può dipendere dall'intera sequenza percettiva osservata fino a quel momento, ma non da qualcosa che non abbia percepito.
```

In termini matematici, diciamo che il comportamento di un agente è descritto dalla **funzione agente** che mappa una data sequenza di percezioni in un'azione.

$$f : \textit{sequenza percettiva} \to \textit{azione}$$

Internamente, la funzione di agente per un agente artificiale sarà implementata da un **programma agente**. È importante tenere distinte queste due idee:
- La funzione agente è una descrizione matematica astratta. 
- Il programma agente è un'implementazione concreta, eseguita all'interno di un sistema fisico.

```ad-note
title: Agente razionale
Un agente razionale è un agente che fa la cosa giusta. Quando un agente si trova in un ambiente, genera una sequenza di azioni in base alle percezioni che riceve. 
- Questa sequenza di azioni fa sì che l'ambiente passi attraverso una sequenza di stati. 
- Se la sequenza è desiderabile, allora l'agente si è comportato bene. 
- Questa nozione di desiderabilità è catturata da criterio di valutazione oggettivo che valuta qualsiasi sequenza di stati dell'ambiente (misura della prestazione).
```

La misura della prestazione è esterna e scelta dal progettista a seconda del problema considerando l’effetto desiderato sull’ambiente.
## Agente Razionale

```ad-def
title: Razionalità
Ciò che è razionale in un dato momento dipende da quattro fattori:
1. La misura di prestazione (che definisce il criterio di successo).
2. La conoscenza pregressa dell'ambiente da parte dell'agente. 
3. Le azioni che l'agente può eseguire (le capacità dell'agente).
4. La sequenza percettiva dell'agente.
```

```ad-def
title: Agente Razionale
Un agente razionale è un agente che, per ogni sequenza di percezioni, compie l’azione che massimizza il valore atteso della misura delle prestazioni, considerando le sue percezioni passate e la sua conoscenza pregressa.
```

Dobbiamo fare attenzione a distinguere tra razionalità e onniscienza. Un agente onnisciente conosce il risultato effettivo delle sue azioni e può agire di conseguenza; ma l'onniscienza è impossibile nella realtà. Non si pretendono perfezione e conoscenza del *“futuro”,* ma massimizzare il risultato atteso.

> La razionalità massimizza le prestazioni attese, mentre la perfezione massimizza le prestazioni effettive.

- Raramente tutta la conoscenza sull’ambiente può essere fornita “a priori” (dal programmatore).
- L’agente razionale deve essere in grado di modificare il proprio comportamento con l’esperienza (le percezioni passate).
- Può migliorare esplorando, apprendendo, aumentando autonomia per operare in ambienti differenti o mutevoli.

Se un agente si affida alle conoscenze pregresse del suo programmatore piuttosto che alle proprie percezioni, si dice che l'agente manca di autonomia.

```ad-def
Un agente è autonomo nella misura in cui il suo comportamento dipende dalla sua capacità di ottenere esperienza (e non dall’aiuto del progettista). Un agente il cui comportamento dipende esclusivamente dalla sua conoscenza pregressa è non autonomo e poco flessibile.
```
## Ambiente
```ad-def
title: Ambiente
Definire un problema per un agente significa caratterizzare l’ambiente in cui l’agente opera (ambiente operativo). I problemi vengono descritti PEAS:
- **P**erformance.
- **E**nvironment.
- **A**ctuators.
- **S**ensors. 
```

e.g Tassista

| Tipo di agente | Performance Measure (prestazione)                                                | Environment                            | Actuators                                                                             | Sensors |
| -------------- | -------------------------------------------------------------------------------- | -------------------------------------- | ------------------------------------------------------------------------------------- | ------- |
| Tassista       | Arrivare a destinazione, affidabile, sicuro, veloce, min. Benzina, max. Profitto | Strada, altri veicoli, pedoni, clienti | Sterzo, acceleratore, freni, frecce, clacson, schermo di interfaccia o sintesi vocale | Telecamere, sensori a infrarossi e sonar, tachimetro, GPS, contachilometri, accelerometro, sensori sullo stato del motore, tastiera o microfono        |

### Proprietà degli ambienti
Possiamo identificare delle caratteristiche per classificare gli ambienti. Queste caratteristiche determinano, in larga misura, come costruire e implementare l'agente. 
Un ambiente può essere:

```ad-note
title: Completamente vs parzialmente osservabile
Se i sensori di un agente gli danno accesso allo stato completo dell'ambiente in ogni momento, allora diciamo che l'ambiente è completamente osservabile. L'ambiente è effettivamente completamente osservabile se i sensori rilevano tutti gli aspetti rilevanti per la scelta dell'azione; gli ambienti completamente osservabili sono convenienti perché l'agente non ha bisogno di mantenere uno stato interno per tenere traccia del mondo. Un ambiente può essere parzialmente osservabile a causa di sensori rumorosi e imprecisi o perché parti dello stato sono semplicemente assenti dai dati dei sensori.
```

```ad-note
title: Agente singolo vs multi-agente 
Un agente che risolve un cruciverba da solo è in un ambiente ad agente singolo, mentre un agente che gioca a scacchi è in un ambiente a due agenti (multi-agente). Nello specifico, un agente A (il tassista, per esempio) deve trattare un oggetto B (un altro veicolo) come un agente, o può essere trattato semplicemente come un oggetto? Il mondo può anche cambiare per eventi, non necessariamente per azioni di agenti.
```

```ad-note
title: Competitivo vs cooperativo 
Competitivo quando abbiamo due agenti che competono, gli scacchi sono quindi un ambiente multi-agente competitivo. Nell'ambiente di guida dei taxi, invece, evitare le collisioni massimizza la misura delle prestazioni di tutti gli agenti, quindi è un ambiente multiagente parzialmente cooperativo. La comunicazione emerge spesso come comportamento razionale negli ambienti multiagente; in alcuni ambienti competitivi, il comportamento randomizzato è razionale perché evita le insidie della prevedibilità.
```

```ad-note
title: Deterministico vs stocastico
Ambiente deterministico se lo stato successivo è completamente determinato dallo stato corrente e dall’azione (e.g scacchi), si parla di ambiente stocastico quando esistono elementi di incertezza con associata probabilità (e.g guida, tiro in porta). Un ambiente è definito non deterministico si tiene traccia di più stati possibili risultato dell’azione (ma non in base ad una probabilità); si pianifica una situazione condizionale, come un automa non deterministico. Si portano avanti più rami degli if, tenendo conto di ognuno di essi.
```

```ad-note
title: Episodico vs sequenziale
L'ambiente viene definito episodico se l’esperienza dell’agente è divisa in episodi atomici indipendenti, in ogni episodio l'agente riceve una percezione e poi esegue una singola azione. In particolare, l'episodio successivo non dipende dalle azioni compiute negli episodi precedenti; quindi pianificare è inutile. Invece in un ambiente sequenziale ogni decisione influenza le successive.
```

```ad-note
title: Statico vs dinamico
In un ambiente statico il mondo non cambia mentre l’agente decide l’azione, non varia nel tempo. Invece un ambiente dinamico cambia nel tempo, va osservata la contingenza. Nel caso di ambiente dinamico, tardare equivale a non agire.
	- **Semidinamico**. Un ambiente viene definito semidinamico se l'ambiente in sé non cambia con il passare del tempo, ma la misura di performance dell'agente lo fa. e.g scacchi con tempo, abbiamo una componente relativa al tempo.
```

```ad-note
title: Discreto vs continuo 
La distinzione discreto/continuo si applica allo stato dell'ambiente, al modo in cui viene gestito il tempo e alle percezioni e azioni dell'agente. Ad esempio, l'ambiente degli scacchi ha un numero finito di stati distinti (escluso l'orologio). Gli scacchi hanno anche un insieme discreto di percezioni e azioni. La guida dei taxi è un problema a stato continuo e a tempo continuo: la velocità e la posizione del taxi e degli altri veicoli passano attraverso una gamma di valori continui e lo fanno in modo fluido nel tempo.

```

```ad-note
title: Noto vs ignoto 
In un ambiente noto, gli esiti (o le probabilità di esito, se l'ambiente è stocastico) per tutte le azioni sono forniti. Ovviamente, se l'ambiente è sconosciuto, l'agente dovrà imparare come funziona per prendere buone decisioni. È possibile che un ambiente noto sia parzialmente osservabile; per esempio, nei giochi di carte in solitario, conosco le regole ma non sono ancora in grado di vedere le carte che non sono ancora state girate. Un ambiente sconosciuto può essere completamente osservabile: in un nuovo videogioco, lo schermo può mostrare l'intero stato del gioco, ma non so ancora cosa fanno i pulsanti finché non li provo.
```

### Simulatore di ambienti
Strumento software che si occupa di:
- Generare stimoli per gli agenti.
- Raccogliere le azioni in risposta.
- Aggiornare lo stato dell’ambiente.
- Attivare altri processi che influenzano l’ambiente.
- Valutare le prestazioni degli agenti.

## Struttura degli agenti
Il compito dell'IA è quello di progettare un programma agente che implementi la funzione di agente - la mappatura dalle percezioni alle azioni. Supponiamo che questo programma venga eseguito su una sorta di dispositivo informatico con sensori e attuatori fisici, che chiamiamo architettura:

$$agente = architettura + programma$$
$$ag:percezioni \to azioni$$

In generale, l'architettura mette a disposizione del programma le percezioni dei sensori, esegue il programma e trasmette le scelte di azione del programma agli attuatori man mano che vengono generate.
oss. il programma dell'agente implementa $ag$.

### Programma agente
I programmi agente che vedremo hanno tutti lo stesso scheletro: prendono in input la percezione corrente dai sensori e restituiscono un'azione agli attuatori. Si noti la differenza tra il programma agente, che prende in input la percezione corrente, e la funzione agente, che prende in input l'intera sequenza di percezioni. Il programma agente prende in input solo la percezione corrente, perché non c'è nient'altro disponibile dall'ambiente; se le azioni dell'agente devono dipendere dall'intera sequenza di percezioni, l'agente dovrà ricordare le percezioni.

- **Programma agente generico**
	```javascript
	function Skeleton_Agent(percept) returns an action 
		static: memory // the agent’s memory of the world 
		memory <- UpdateMemory(memory, percept) 
		action <- ChooseBestAction(memory)
		memory <- UpdateMemory(memory, action) 
		return action
	```
- **Programma agente basato su tabella** 
La scelta dell’azione è un accesso a una «tabella» che associa un’azione ad ogni possibile sequenza di percezioni. Non vediamo nemmeno il codice perché è una soluzione poco pratica per i seguenti motivi:
	1. Dimensione: Per giocare a scacchi tabella con un numero di $righe >> 10^{80}$  numero ingestibile.
	2. Difficile da costruire.
	3. Nessuna autonomia.
	4. Di difficile aggiornamento, apprendimento complesso.

n.b: in IA vogliamo realizzare agenti razionali con programma “compatto”, per praticità e per evitare information bottleneck.
### Agenti reattivi semplici
Il tipo più semplice di agente è l'agente reattivo semplice. Questi agenti selezionano le azioni sulla base della percezione corrente, ignorando il resto della storia percettiva.

![[agente_reattivo_semplice.png]]

I comportamenti reattivi semplici si verificano anche in ambienti complessi. Il conducente di un taxi automatizzato, se l'auto che precede frena e le sue luci dei freni si accendono, lo nota e inizia a frenare. In altre parole, viene effettuata un'elaborazione dell'input visivo per stabilire la condizione che chiamiamo "L'auto davanti sta frenando". Poi, questo innesca una connessione stabilita nel programma dell'agente per l'azione "inizia la frenata". Chiamiamo tale connessione una regola **condizione-azione**:
$$\bf{if} \textit{ car-in-front-is-braking } \bf{then} \textit{ initiate-braking }$$

```javascript
function Agente_Reattivo_Semplice(percezione) returns azione
	persistent: regole // un insieme di regole condizione-azione (if-then)
	stato <- InterpretaInput(percezione)
	regola <- RegolaCorrispondente(stato, regole)
	azione <- regola.Azione
	return azione
```

La funzione `InterpretaInput` genera una descrizione astratta dello stato corrente a partire dalla percezione, mentre la funzione `RegolaCorrispondente` restituisce la prima regola dell'insieme di regole che corrisponde alla descrizione dello stato data.
L'agente descritto funziona solo se la decisione corretta può essere presa sulla base della sola percezione corrente, cioè solo se l'ambiente è completamente osservabile. Anche un po' di inosservabilità può causare seri problemi.

### Agenti basati su modello
Il modo più efficace per gestire l'osservabilità parziale è che l'agente tenga traccia della parte del mondo che non può vedere ora. In altre parole, l'agente dovrebbe mantenere una sorta di stato interno che dipende dalla storia delle percezioni e quindi riflette almeno alcuni degli aspetti non osservati dello stato corrente. L'aggiornamento di queste informazioni sullo stato interno con il passare del tempo richiede due tipi di conoscenze da codificare nel programma dell'agente: 
1. Abbiamo bisogno di alcune informazioni su come il mondo si evolve indipendentemente dall'agente, ad esempio che un'auto in sorpasso sarà generalmente più vicina rispetto a un momento prima. 
2. Abbiamo bisogno di informazioni su come le azioni dell'agente stesso influenzano il mondo. 

Questa conoscenza su "come funziona il mondo" è chiamata *modello* del mondo, un'agente che usa questo tipo di modello può essere descritto come segue:

```javascript
function AgenteBasatoSuModello(percezione) returns azione  
	persistent: stato   // una descrizione dello stato corrente
				modello // conoscenza del mondo  
				regole  // insieme di regole condizione-azione
				azione  // l’azione più recente

	stato <- AggiornaStato(stato, azione, percezezione, modello)
	regola <- RegolaCorrispondente(stato, regole)
	azione <- regola.Azione 
	return azione
```

La parte interessante è la funzione `AggiornaStato`, responsabile della creazione della descrizione del nuovo stato interno. Indipendentemente dal tipo di rappresentazione utilizzata, raramente l'agente è in grado di determinare esattamente lo stato corrente di un ambiente parzialmente osservabile.

![[agente_basato_modello.png]]

La box con l'etichetta "What the world is right now" rappresenta la "migliore guess" dell'agente. Pertanto, l'incertezza sullo stato attuale può essere inevitabile, ma l'agente deve comunque prendere una decisione.

### Agenti con obiettivo
Sapere qualcosa sullo stato attuale dell'ambiente non è sempre sufficiente per decidere cosa fare. e.g a un incrocio stradale, il taxi può girare a sinistra, a destra o proseguire dritto. La decisione corretta dipende da dove il taxi sta cercando di arrivare.

![[agente_obiettivo.png]]

Quindi oltre alla descrizione dello stato attuale, l'agente ha bisogno di una sorta di informazione sull'obiettivo che descriva le situazioni desiderabili. Il programma agente può combinare queste informazioni con il modello (le stesse utilizzate nell'agente reattivo basato sul modello) per scegliere le azioni che raggiungono l'obiettivo. È necessario pianificare una sequenza di azioni per raggiungere l'obiettivo (goal).
- Sono guidati da un obiettivo nella scelta dell’azione.
- A volte l’azione migliore dipende da qual è l'obiettivo da raggiungere.
- Devono pianificare una sequenza di azioni per raggiungere l’obiettivo.
- Meno efficienti ma più flessibili di un agente reattivo (l'obiettivo può cambiare, non è già codificato nelle regole)

### Agenti con valutazione di utilità
Agenti di questo tipo considerano l'obiettivo e l'utilità di quest'ultimo. 

![[agente_utilità.png]]

- Vi sono più obiettivi alternativi (o più modi per raggiungerlo).
	- l’agente deve decidere verso quali di questi muoversi.
	- necessaria di una funzione di utilità (che associa ad uno stato obiettivo un numero reale).

La funzione di utilità di un agente è essenzialmente espressione della misura di performance. Se la funzione di utilità interna e la misura di performance esterna sono in accordo, allora un agente che sceglie azioni per massimizzare la propria utilità sarà razionale secondo la misura di performance esterna.

- Alcuni obiettivi più facilmente raggiungibili di altri
	- la funzione di utilità tiene conto anche della probabilità di successo (e/o di ciascun risultato): utilità attesa (o in media)

e.g Taxi: stessa destinazione ma diversi tempi (per diverse strade) per raggiungerla.

### Agenti che apprendono
L'apprendimento consente all'agente di operare in ambienti inizialmente sconosciuti e di diventare più competente di quanto le sue sole conoscenze iniziali potrebbero consentire.
Un agente di apprendimento può essere suddiviso in quattro componenti concettuali:
1. **Componente di apprendimento**. Produce cambiamenti al programma agente, migliora prestazioni, adattando i suoi componenti, apprendendo dall’ambiente.
2. **Elemento esecutivo**. Il programma agente (visto sinora per decider le azioni).
3. **Elemento critico**. Osserva e dà feedback sul comportamento.
4. **Generatore di problemi**. Suggerisce nuove situazioni da esplorare.

![[agente_apprende.png]]

L'elemento critico comunica alla componente di apprendimento quanto bene si sta comportando l'agente rispetto alla performance fissata. L'elemento critico è necessario perché dalle sole percezioni l'agente non capisce se l'azione ha avuto successo o meno.

### Tipi di rappresentazione
![[tipi_rappresentazione.png]]

Vi sono tre modi per rappresentare gli stati e le transizioni tra di essi. 
(a) Rappresentazione atomica: uno stato (come B o C) è una scatola nera senza struttura interna; (b) Rappresentazione fattorizzata: uno stato consiste in un vettore di valori di attributi; i valori possono essere booleani, a valori reali o uno di un insieme fisso di simboli. 
(c) Rappresentazione strutturata: uno stato comprende oggetti, ognuno dei quali può avere attributi propri e relazioni con altri oggetti.

