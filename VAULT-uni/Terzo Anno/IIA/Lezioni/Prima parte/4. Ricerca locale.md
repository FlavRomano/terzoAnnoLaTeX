Table of contents

1. [[#Ricerca locale|Ricerca locale]]
	1. [[#Ricerca locale#Panorama dello spazio degli stati|Panorama dello spazio degli stati]]
	1. [[#Ricerca locale#Ricerca in salita (Hill climbing, steepest-ascent/descent)|Ricerca in salita (Hill climbing, steepest-ascent/descent)]]
		1. [[#Ricerca in salita (Hill climbing, steepest-ascent/descent)#Codice|Codice]]
		1. [[#Ricerca in salita (Hill climbing, steepest-ascent/descent)#Problema delle 8 regine (formulazione a stato completo)|Problema delle 8 regine (formulazione a stato completo)]]
		1. [[#Ricerca in salita (Hill climbing, steepest-ascent/descent)#Miglioramenti|Miglioramenti]]
	1. [[#Ricerca locale#Ricerca local beam|Ricerca local beam]]
	1. [[#Ricerca locale#Beam search stocastica|Beam search stocastica]]
	1. [[#Ricerca locale#Algoritmi genetici/evolutivi|Algoritmi genetici/evolutivi]]
		1. [[#Algoritmi genetici/evolutivi#Nascita di un figlio|Nascita di un figlio]]
1. [[#Ricerca locale in spazi continui|Ricerca locale in spazi continui]]
1. [[#Ambienti più realistici|Ambienti più realistici]]
	1. [[#Ambienti più realistici#Caso studio: Aspirapolvere non deterministico|Caso studio: Aspirapolvere non deterministico]]
	1. [[#Ambienti più realistici#Alberi di ricerca AND-OR|Alberi di ricerca AND-OR]]


```ad-def
title: Agenti risolutori "classici"
Gli agenti risolutori di problemi "classici" assumono:
- Ambienti completamente osservabili e deterministici
- Si trovano nelle condizione di produrre offline una sequenza di azioni che può essere eseguito senza imprevisti per raggiungere l'obiettivo.
```

La ricerca sistematica (o euristica) nello spazio di stati è troppo costosa $\implies$ metodi di ricerca locale.

## Ricerca locale
```ad-warning
title: Assunzioni per la ricerca locale
Fin'ora abbiamo visto algoritmi che esplorano gli spazi di ricerca alla ricerca di un goal e restituiscono un cammino soluzione.
Tuttavia a volte lo stato goal è la soluzione del problema.
```

Gli algoritmi di ricerca locale sono adatti per problemi in cui:
- La sequenza di azioni non è importante, è importante unicamente lo stato goal.
- Tutti gli elementi della soluzione sono nello stato ma alcuni vincoli sono violati. Ci interessa ottenere la soluzione, non il path.

```ad-def
title: Algoritmi di ricerca locale
- Non sono sistematici
- Tengono traccia solo del nodo corrente e si spostano su nodi adiacenti (al contrario di un algoritmo globale).
- Non tengono traccia dei cammini (non servono in uscita).
	1. Sono efficienti in termini di occupazione di memoria
	2. Possono trovare soluzioni ragionevoli anche in spazi molto grandi e infiniti (e.g spazi continui).
- Utili per risolvere problemi di ottimizzazione:
	- Stato migliore secondo una funzione obiettivo $f$.
	- Stato di costo minore (ma non il path, cerchiamo lo stato di costo minore, non il percorso di costo minore).

e.g minimizzare il numero di regine sotto attacco.

```

### Panorama dello spazio degli stati
Uno stato ha una posizione sulla superficie e un'altezza che corrisponde al valore della funzione di valutazione (funzione obiettivo).

![[panorama_sstati.png]]

$$f =\text{euristica di costo della funzione obiettivo}$$ Ci spostiamo di stato in stato e ogni volta viene rivalutata la funzione obiettivo. Un algoritmo provoca movimento sulla superficie.
- Vogliamo trovare l'avvallamento più basso (e.g minimizzare il costo) o il picco più alto (e.g massimo di un obiettivo).

### Ricerca in salita (Hill climbing, steepest-ascent/descent)
L'algoritmo di ricerca hill-climbing (versione steepest-ascent) non è altro che un ciclo che si muove continuamente in direzione di un valore crescente, cioè in salita. 
- Termina quando raggiunge un "picco" in cui nessun vicino ha un valore superiore. 
- L'algoritmo non mantiene un albero di ricerca, quindi la struttura dei dati per il nodo corrente deve registrare solo lo stato e il valore della funzione obiettivo. 
- L'Hill climbing non guarda oltre i vicini immediati dello stato corrente. È come cercare di trovare la cima dell'Everest in una fitta nebbia mentre si soffre di amnesia.

Abbiamo diverse implementazioni di questo algoritmo di ricerca che si differenziano per il modo in cui viene scelto un nodo tra quelli generati (in modo tale da migliorare la valutazione dello stato attuale):
- Seleziono il migliore $\implies$ Hill climbing a salita ripida.
- Seleziono uno a caso (tra quelli che salgono) $\implies$ Hill climbing stocastico.
- Seleziono il primo generato $\implies$ Hill climbing con prima scelta.

Se non ci sono stati successori migliori, allora l'algoritmo termina con fallimento.

```ad-note
title: Greediness
L'Hill Climbing è talvolta chiamato ricerca locale greedy, in quanto afferra uno stato di buon vicinato senza pensare a dove andare dopo. L'Hill Climbing fa spesso rapidi progressi verso una soluzione, perché di solito è abbastanza facile migliorare uno stato negativo. 
```

```ad-warning
title: Quando si interrompe l'hill climbing?
- **Massimi locali**. Un massimo locale è un picco che è più alto di ciascuno dei suoi stati vicini ma più basso del massimo globale. Gli algoritmi di Hill-climbing che raggiungono l'intorno di un massimo locale saranno attratti verso l'alto in direzione del picco, ma saranno poi bloccati senza poter andare da nessuna parte.
- **Creste**. Le creste danno luogo a una sequenza di massimi locali che è molto difficle da percorrere per gli algoritmi greedy.
- **Plateaux**. Un plateau è un'area piatta dello spazio degli stati. Può essere un massimo locale piatto, da cui non esiste un'uscita in salita da cui è possibile progredire. Una ricerca di tipo hill-climbing potrebbe perdersi su un plateau.
```
#### Codice
```python
def hill_climbing(problem):
	current = Node(problem.initial_state)
	while True:
		vicini = [current.child_node(problem, action) 
			for action in problem.actions(current.state)]
		if not vicini:
			# current non ha successori
			# esci e ritorna current
			break
		# scegli il vicino con valore più alto (sulla funzione problem.value)
		vicino = (sorted(vicini, key = lambda x: problem.value(x), reverse = True))[0]
		if problem.value(vicino) <= problem.value(current):
			# non miglioro lo stato
			break
		# il vicino è migliore, aggiorno current
		current = vicino
	return current
	
```

#### Problema delle 8 regine (formulazione a stato completo)
Gli algoritmi di ricerca locale utilizzano in genere una formulazione a stati completi, in cui ogni stato ha 8 regine sulla scacchiera, una per colonna. I successori di uno stato sono tutti i possibili stati generati dallo spostamento di una singola regina in un'altra casella della stessa colonna (quindi ogni stato ha $8 \cdot 7 = 56\;successori$). La funzione di costo euristico $h$ (stima euristica del costo $f$) è il numero di coppie di regine che si attaccano a vicenda, direttamente o indirettamente. 

![[8regine_hillclimbing_ex1.png]]
- Il minimo globale di questa funzione è zero, che si verifica solo in presenza di soluzioni perfette. La Figura mostra uno stato con $h = 17$, all'interno delle caselle mostra i valori di tutte le coppie di regine che si attaccano reciprocamente. La figura mostra anche i valori di tutti i suoi successori, con i migliori successori che hanno h=12. Gli algoritmi di Hill-climbing scelgono tipicamente in modo casuale tra l'insieme dei migliori successori, se ce n'è più di uno.

![[8regine_hillclimbing_ex2.png]]
- In questo caso abbiamo $h = 1$, tutti gli stati successori non migliorano la soluzione.
- Per le 8 regine l'algoritmo si blocca l'$86\%$ delle volte.
- Però funziona rapidamente, impiegando in media solo 4 passi quando riesce e 3 quando si blocca. Non male per uno spazio di stati con $8^8 \sim 17$ milioni di stati.

![[8regine_hillclimbing_ex3.png]]
- In questo caso in sole 3 mosse siamo riusciti a minimizzare la funzione obiettivo, passando da $h = 5$ (perché si stanno attaccando 5 coppie) fino a 0 (minimo assoluto).

#### Miglioramenti
1. Consentire, un numero limitato di, mosse laterali. Ossia ci si ferma per minore stretto $<$ nell'algoritmo piuttosto che per $\leq$, quindi continua anche a parità di $h$; questo perché magari dopo un certo plateau si può salire ancora (rilassiamo un vincolo).
2. **Hil-climbing stocastico**. si sceglie a caso tra le mosse in salita (magari tenendo conto della pendenza).
	- Converge più lentamente ma a volte trova soluzioni migliori.
3. **Hill-climbing con prima scelta**. può generare le mosse a caso, uno alla volta, fino a trovarne una migliore dello stato corrente (si prende solo il primo che migliora).
	- Come la stocastica ma utile quando i successori sono molti (nell'ordine delle migliaia o più) evitando una scelta tra tutti.
4. **Hill-climbing con riavvio casuale** (random restart): *"Se all'inizio non ci riuscite, provate e riprovate"*. Si tratta di una serie di ricerche di tipo hill-climbing a partire da stati iniziali generati in modo casuale, fino a trovare un goal.
	- Se la probabilità di successo è $p\implies$ saranno necessarie in media $$\dfrac{1}{p}\text{ ripartenze}$$ per trovare la soluzione.
	- Questa variante è nota per essere tendenzialmente completa (e.g insistendo si generano tutte).
	- Il suo successo dipende molto dalla forma del panorama degli stati:
		- Panorama frastagliato $\implies$ molti massimi locali $\implies$ si inceppa facilmente.

```ad-def
title: Tempra simulata
L'algoritmo di tempra simulata (Simulated annealing) combina l'hill-climbing con una scelta stocastica (non del tutto casuale per via dell'efficienza). In soldoni è una sofisticazione dell'approccio stocastico.
- Il nome è analogo al processo di tempra dei metalli in metallurgia:
> Se c’è un difetto reticolare (stato che non ci piace) riscaldano il metallo (molto caos) per poi raffreddarlo pian piano per raggiungere la configurazione desiderata.

```

```ad-def 
title: Tempra simulata (per ascesa)
Ad ogni passo si sceglie un successore $u'$ casuale:
- Se migliora lo stato corrente $\implies u'$ viene espanso
- Altrimenti, caso in cui $$\Delta E = f(u') - f(u) \leq 0$$ quel nodo viene scelto con probabilità $$p = \exp{(\Delta E / T)}, \;\; 0\leq p \leq 1$$ Cioè, se siamo nel caso peggiorativo o uguale possiamo proseguire con un successore con una certa probabilità, altrimenti no.

 Come possiamo notare:
 - $p$ è inversamente proporzionale al peggioramento, infatti se la mossa peggiora molto (cioè $\Delta E$ diventa molto negativo) allora la $p$ si abbassa (probabilità minore).
 - La temperatura $T$ decresce col progradire dell'algoritmo (quindi anche $p$) secondo una strategia definita:
> Col progredire rende improbabili le mosse peggiorative.
> - Se $T$ alta (situazione iniziale) abbiamo $e^x$ con $x$ molto vicino a 0, quindi la probabilità è molto vicina a 1. Accetto configurazioni molto più facilmente.
> - Se $T$ bassa $e^x$ tende a infinito quindi diventa molto meno probabile.
```

```ad-note
title: Analisi Tempra Simulata
La probabilità $p$ di una mossa in discesa diminuisce col tempo e l'algoritmo si comporta sempre di più come Hill Climbing (con temperature basse, vicino a zero, si comporta come Hill Climbing).
- Se $T$ viene decrementato abbastanza lentamente con $p\to1$ si raggiunge la soluzione ottimale.
```

```ad-note
title: Parametri tempra simulata
Valore iniziale e decremento di $T$ sono parametri.
- I valori per $T$ vengono determinati sperimentalmente, inizialmente $$T \text{ tale che per valori medi di }\Delta E \text{ vale che }\exp{(\Delta E / T)} \sim 0.5$$
```
### Ricerca local beam
È la versione locale della beam search: si tengono in memoria $k$ stati, anziché uno solo.
- Ad ogni passo si generano i successori di tutti i $k$ stati.
	- Se si trova un goal ci si ferma.
	- Altrimenti si prosegue con i $k$ migliori tra questi successori.

### Beam search stocastica
Si introduce un elemento di casualità, come in un processo di selezione naturale (in modo tale da diversificare le future generazioni).
- Nella variante stocastica della local beam, si scelgono $k$ successori ma con probabilità maggiore per i migliori (non necessariamente si scelgono i migliori).

$$organismo \equiv stato$$
$$\textit{popolazione di individui} \equiv stati$$
$$progenie \equiv successori$$
$$fitness \text{(il valore della f)} \equiv idoneità$$

### Algoritmi genetici/evolutivi
```ad-def
title: Idea
Sono varianti della beam search stocastica in cui gli stati successori sono ottenuti combinando due stati genitore (anziché per evoluzione).
> Non si evolve una generazione dietro l’altra, due genitori procreano nuovi successori accoppiandosi.
```

```ad-def
title: Funzionamento
Abbiamo una popolazione iniziale:
- $k$ stati/individui vengono generati casualmente.
- Ogni individuo è rappresentato come una stringa. e.g `"24748552"` posizione nelle colonne nel problema delle 8 regine.
- Gli individui sono valutati da una funzione di fitness. e.g $\#\text{coppie di regine che non si attaccano}$.
- Si selezionano gli individui per gli accoppiamenti con una probabilità proporzionale alla fitness.
- Le coppie danno vita alla generazione successiva.
	- Combinando materiale genetico (**crossover**)
	- Con un meccanismo aggiuntivo di mutazione genetica casuale.
- La popolazione ottenuta dovrebbe essere migliore.
- La cosa si ripete fino a ottenere stati abbastanza buoni (stati obiettivo) o finché si raggiunge un "plateau" a livello di miglioramenti.
```

e.g:

![[8regine_algGenetico_ex.png]]

> Per ogni coppia (scelta con probabilità proporzionale alla fitness) viene: 
> - scelto un punto di crossing over e vengono generati due figli scambiandosi pezzi (del DNA).
> - in seguito viene effettuata una mutazione casuale che dà luogo alla prossima generazione.
> 
> La fitness progressivamente tenderà a favorire generazioni migliori.

Quindi:

- Tendenzialmente prendiamo le configurazioni con miglior fitness. 
- La zona di crossing over viene utilizzata per discriminare quali “geni” prendere da un genitore e quali dall’altro, così da poter sviluppare un figlio che ha le migliori caratteristiche di entrambi. 
- La mutazione avviene casualmente.

#### Nascita di un figlio

![[nascita_figlio_scacchi.png]]

- Le parti chiare sono passate al figlio.
- Le parti grigie si perdono.
- Se i genitori sono molto diversi anche i nuovi stati sono diversi.
- All'inizio vi sono spostamenti maggiori che poi si raffinano.
	- Maggiori perché c’è molta eterogeneità, man mano che si va’ avanti con il fitness ci saranno meno spostamenti.

## Ricerca locale in spazi continui
Molti casi reali hanno spazi di ricerca continua, dove lo stato è descritto da variabili continue del vettore $x$: $$x = \pmatrix{x_1,\ldots,x_n}$$ e.g la posizione dei movimenti in spazio 3D è data da $$x = \pmatrix{x_1,x_2,x_3}$$

- Se la $f$ è continua e differenziabile, possiamo cercare minimo o massimo attraverso il gradiente: fornisce la direzione di massima pendenza nel punto. Data una $f$ obiettivo in $\mathbb{R^3}$ $$\nabla f = \left( \dfrac{\partial f}{\partial x_1}, \dfrac{\partial f}{\partial x_2}, \dfrac{\partial f}{\partial x_3} \right)$$
- Data un'espressione localmente corretta per il gradiente, possiamo eseguire l'hill-climbing iterativo aggiornando lo stato corrente secondo la formula $$x_{\text{new}} = x \pm \eta \cdot \nabla f(x)$$ 
	- Uso $+$ per salire (massimizzazione) e $-$ per scendere (minimizzazione).
	- Lo scalare $\eta$ è una costante chiamata *step size*.
		- Se $\eta$ è troppo piccolo $\implies$ servono molti passi.
		- Se $\eta$ è troppo grande $\implies$ la ricerca potrebbe superare il massimo. (analogo viceversa per il minimo)

e.g Discesa verso il minimo, $f(x) = x^2$.
$$f(x) = x^2 \implies f'(x) = 2x$$

Vogliamo minimizzare la funzione obiettivo, quindi discendiamo utilizzando il gradiente verso il minimo (uso $-$)
$$x_{\text{new}} = x - \eta\nabla f(x) \underset{\mathbb{R}^1}{\implies} x_\text{new} = x - \eta f'(x)$$
$$x=1 \;\therefore f'(x) = 2x = 4 \implies \text{mi devo spostare di }-(\eta \cdot4)$$
- Prendendo $\eta = 0.2$ otteniamo $$-\eta f'(x) = -0.2 \cdot 4 = -0.8$$
- Quindi ci avviciniamo al minimo andando (a sinistra) verso il punto $$x_\text{new} = 2-0.8 = 1.2$$

![[gradiente_ex.png]]

## Ambienti più realistici
In precedenza abbiamo assunto che l'ambiente sia completamente osservabile e deterministico e che l'agente conosca gli effetti di ogni azione. 
- Pertanto, l'agente può calcolare esattamente quale stato risulta da qualsiasi sequenza di azioni e sa sempre in quale stato si trova.
- Le sue percezioni non forniscono nuove informazioni dopo ogni azione, anche se ovviamente gli comunicano lo stato iniziale.

Quando l'ambiente è parzialmente osservabile o non deterministico, le percezioni diventano utili.
- In un ambiente parzialmente osservabile, ogni percezione aiuta a restringere l'insieme dei possibili stati in cui l'agente potrebbe trovarsi, facilitando così il raggiungimento dei suoi obiettivi. 
- Quando l'ambiente è non deterministico, le percezioni dicono all'agente quale dei possibili esiti delle sue azioni si è effettivamente verificato.
- In ambo i casi, le percezioni future possono essere determinate in anticipo e da esse dipenderanno le azioni future dell'agente.

Quindi la soluzione a un problema non è una sequenza, ma un piano di contigenza (o "strategia") che specifica cosa fare a seconda delle percezioni ricevute.
> Faccio questa cosa sapendo che si verificherà quest'altra.

### Caso studio: Aspirapolvere non deterministico
Consideriamo un aspirapolvere imprevedibile in cui ci sono più stati possibili a seguito di un'azione.
```ad-note
title: Comportamento
- Se aspira in una stanza sporca:
	- La pulisce.
	- Ogni tanto pulisce anche una stanza adiacente.
- Se aspira in una stanza pulita:
	- Non succede nulla.
	- Ogni tanto rilascia sporco.
```

```ad-note
title: Modello precedente
- Stati: Lo stato è determinato sia dalla posizione dell'agente che dalla posizione dello sporco. L'agente si trova in una delle due posizioni, ognuna delle quali può contenere o meno dello sporco.
- Stato iniziale: Qualsiasi stato può essere designato come stato iniziale.
- Azioni: In questo semplice ambiente, ogni stato ha solo tre azioni: `{Sinistra, Destra e
Aspirare`.
- Modello di transizione: Le azioni hanno gli effetti previsti, tranne il fatto che muoversi a sinistra nel quadrato più a sinistra, muoversi a destra nel quadrato più a destra e aspirare in un quadrato pulito non hanno alcun effetto.
- Test dell'obiettivo: Verifica se tutte le caselle sono pulite.
- Costo del percorso: Ogni passo costa 1, quindi il costo del percorso è il numero di passi nel percorso.
```

```ad-warning
title: Cambiamenti rispetto al precedente modello
- Il modello di transizione restituisce un **insieme di stati** (l'agente non sa in quale si troverà).
- Il piano di contigenza sarà un piano condizionale (in cui possono essere presenti cicli). 
```

![[aspirapolvere_nonDet.png]]

e.g `Risultati(Aspira, 1) = {5, 7}`

```python
# Piano possibile
if stato is 5:
	return [Destra, Aspira]
return []
```

La soluzione è data da una sequenza di azione per ogni piano (albero).

### Alberi di ricerca AND-OR
La domanda sorge spontanea: come trovare soluzioni contingenti a problemi non deterministici.
- Iniziamo col costruire alberi binari di ricerca, ma con caratteristiche diverse rispetto a quelle che troveremmo in un ambiente deterministico. La ramificazione è introdotta:
	- Dalle scelte dell'agente in ogni stato. Chiamiamo questi nodi **NODI OR**. e.g nell'aspirapolvere non deterministica in un nodo OR l'agente sceglie `Sinistra, Destra, Aspira`.
	- Dalla scelta dell'esito dell'ambiente per ogni azione (più stati possibili). Chiamiamo questo nodi **NODI AND**. e.g nell'aspirapolvere non deterministica l'azione `Aspira` nello stato 1 porta a uno stato dell'insieme `{5,7}`, quindi l'agente deve trovare un piano per lo stato 5 e uno per lo stato 7.

L'alternarsi di questi due nodi forma un **Albero AND-OR**.
```ad-note
title: Soluzione per un albero AND-OR
Una soluzione per un problema di ricerca AND-OR è un sottoalbero che ha:
1. Un nodo obiettivo **in ogni foglia**.
2. Specifica un'azione in ciascuno dei suoi nodi OR.
3. Include tutti gli archi uscenti da nodi AND (cioè tutte le contingenze).
```

![[abr_aspirapolvereNonDet.png]]

$$\text{Piano soluzione}= \texttt{[Aspira, Stato==5 ? [Destra, Aspira] : []}]$$

- Com'è possibile osservare gli stati sono nodi OR in cui l'agente decide che azione intraprendere.
- I nodi AND sono cerchi in cui ogni arco uscente  corrisponde a una diversa contingenza.